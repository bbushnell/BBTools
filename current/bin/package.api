#version 1
#package bin
#generated 2025-09-06T20:35:22

<class AbstractRefiner>
*Abstract base class for bin refinement strategies.
*Accepts a potentially impure bin and returns null for no change,
*or a list of refined bins if beneficial splits are found.
*@author Brian Bushnell & UMP45
*@date June 20, 2025
abstract class AbstractRefiner

#Fields
*Algorithm type constants for supported refinement strategies
public static final int CRYSTAL=0

*Algorithm type constants for supported refinement strategies
public static final int GRAPH=1

*Algorithm type constants for supported refinement strategies
public static final int EVIDENCE=2

*Algorithm type constants for supported refinement strategies
public static final int ENSEMBLE=3

*Human-readable names corresponding to algorithm type constants
public static final String types={"CRYSTAL","GRAPH","EVIDENCE","ENSEMBLE"}

*Default refinement algorithm type used when no specific type is requested
public static int DEFAULT_TYPE=CRYSTAL


#Methods
*Attempts to refine/split the given bin.
*@param input Potentially impure cluster to analyze
*@return null if no refinement recommended, or ArrayList of 2+ bins if split beneficial
ArrayList<Bin> refine(Bin input)

ArrayList<IntHashSet> refineToIntSets(Bin input)

*Validates that a proposed split actually improves cluster quality.
*Override this for custom validation logic.
protected boolean isSplitBeneficial(Bin original, ArrayList<Bin> splits)

*Factory method to create a refiner instance using default type and parameters.
*@param oracle Truth data provider for supervised learning and validation
*@return Configured refiner instance using DEFAULT_TYPE with default parameters
public static AbstractRefiner makeRefiner(Oracle oracle)

*Factory method to create a refiner instance with specified type and default
*parameters.
*@param oracle Truth data provider for supervised learning and validation
*@param type Refinement algorithm type (CRYSTAL, GRAPH, EVIDENCE, or ENSEMBLE)
*@return Configured refiner instance using default parameters for the specified type
public static AbstractRefiner makeRefiner(Oracle oracle, int type)

*Primary factory method for creating configured refiner instances with custom
*parameters.
*Instantiates the appropriate concrete refiner implementation based on the
*specified type:
*- CRYSTAL: CrystalChamber using crystallization-based clustering
*- GRAPH: GraphRefiner using assembly graph community detection
*- EVIDENCE: EvidenceRefiner using DBSCAN clustering on coverage/composition
*features
*- ENSEMBLE: EnsembleRefiner combining multiple methods with consensus voting
*@param oracle Truth data provider for supervised learning, validation, and ground
*truth comparison
*@param type Algorithm type constant (CRYSTAL, GRAPH, EVIDENCE, or ENSEMBLE)
*@param params Type-specific parameter object or null for defaults
*@return Fully configured refiner instance ready for bin processing
*@throws RuntimeException if type is not recognized
public static AbstractRefiner makeRefiner(Oracle oracle, int type, RefinerParams params)

public static int findType(String s)

</class AbstractRefiner>
<class AbstractRefiner.RefinerParams>
*Abstract base class for refiner-specific parameter configuration objects.
*Provides a common interface for passing algorithm-specific parameters to refiner
*instances
*while maintaining type safety through concrete subclasses. Each refiner
*implementation
*defines its own parameter class extending this base to specify tunable algorithm
*parameters
*such as clustering thresholds, iteration limits, and random seeds.
*Supports deep copying to enable parameter mutation experiments without affecting
*original configurations.
public static abstract class AbstractRefiner.RefinerParams

#Methods
*Creates a deep copy of this parameter configuration
public RefinerParams copy()

</class AbstractRefiner.RefinerParams>
<class AbstractRefiner.GraphRefinerParams>
*Configuration parameters for graph-based bin refinement using community detection
*algorithms.
*Controls the behavior of GraphRefiner which analyzes assembly graph connectivity
*patterns
*to identify natural breakpoints between different organisms that were incorrectly
*binned together.
*Uses iterative community detection algorithms such as Louvain or Label Propagation
*to partition
*contigs based on graph edge weights derived from paired-end links, shared k-mer
*content, or other
*connectivity evidence.
public static class AbstractRefiner.GraphRefinerParams

#Fields
*Minimum edge weight threshold for including connections in community detection (0.0-1.0)
public float minEdgeWeight=0.3f

*Maximum iterations for convergence in iterative community detection algorithms
public int maxIterations=50

*Random seed for reproducible community detection results
public long seed=42


#Methods
*Default constructor using standard parameter values
public GraphRefinerParams()

*Constructor with custom parameter values.
*@param minEdgeWeight Minimum edge weight threshold for community detection
*@param maxIterations Maximum iterations for algorithm convergence
*@param seed Random seed for reproducible results
public GraphRefinerParams(float minEdgeWeight, int maxIterations, long seed)

@Override public RefinerParams copy()

</class AbstractRefiner.GraphRefinerParams>
<class AbstractRefiner.EvidenceRefinerParams>
*Configuration parameters for evidence-based bin refinement using DBSCAN density clustering.
*Controls the behavior of EvidenceRefiner which applies DBSCAN clustering to
*multi-dimensional
*feature vectors derived from contig properties such as coverage profiles,
*tetranucleotide frequencies,
*GC content, and other compositional signatures. Identifies dense clusters of similar
*contigs
*that likely originate from the same organism, revealing contamination or chimeric bins.
*DBSCAN parameters determine the sensitivity and specificity of cluster detection in
*high-dimensional
*feature space where euclidean distance represents biological similarity between
*contigs.
public static class AbstractRefiner.EvidenceRefinerParams

#Fields
*DBSCAN epsilon parameter - maximum distance between contigs in same cluster
public float epsilon=0.4f

*DBSCAN minPoints parameter - minimum contigs required to form dense cluster
public int minPoints=3

*Minimum cluster size threshold for accepting a refined bin
public int minClusterSize=2

*Random seed for reproducible clustering results
public long seed=123


#Methods
*Default constructor using standard DBSCAN parameter values
public EvidenceRefinerParams()

*Constructor with custom DBSCAN parameter values.
*@param epsilon Maximum euclidean distance between contigs in same cluster
*@param minPoints Minimum contigs required to form a dense cluster
*@param minClusterSize Minimum cluster size for accepting refined bins
*@param seed Random seed for reproducible results
public EvidenceRefinerParams(float epsilon, int minPoints, int minClusterSize, long seed)

@Override public RefinerParams copy()

</class AbstractRefiner.EvidenceRefinerParams>
<class AbstractRefiner.EnsembleRefinerParams>
public static class AbstractRefiner.EnsembleRefinerParams

#Fields
*Minimum consensus threshold for accepting ensemble splits (0.0-1.0)
public float consensusThreshold=0.6f

*Minimum number of refinement methods that must agree on a split
public int minMethodsAgreeing=2

*Random seed for reproducible ensemble results
public long seed=999

*Parameter configuration for graph-based refinement component
public GraphRefinerParams graphParams

*Parameter configuration for evidence-based refinement component
public EvidenceRefinerParams evidenceParams


#Methods
*Default constructor initializing component parameters with standard values
public EnsembleRefinerParams()

*Constructor with custom consensus parameters using default component parameters.
*@param consensusThreshold Minimum consensus threshold for accepting splits
*@param minMethodsAgreeing Minimum number of methods that must agree
*@param seed Random seed for reproducible results
public EnsembleRefinerParams(float consensusThreshold, int minMethodsAgreeing, long seed)

@Override public RefinerParams copy()

</class AbstractRefiner.EnsembleRefinerParams>
<class AdjustEntropy>
public class AdjustEntropy

#Fields
float min

float mid

float max

private static String fnameLoaded=null

public static int kLoaded=0

public static int wLoaded=0

private static float[] entropyArray=null


#Methods
public static void main(String[] args)

static void processFile(String fname, EntropyTracker et)

static float maxEntropy(float gc, float[] array)

public static void load()

public static void load(int k, int window)

private static void setEntropyFile(String fname)

private static float[] loadEntropyFile(String fname)

*Returns entropy as a fraction of random entropy for this GC level
static float compensate(float gc, float entropy)

float randomSequenceEntropy(int len, float gc, EntropyTracker et, int trials)

static float randomSequenceEntropy(int len, float gc, EntropyTracker et)

static byte[] randomSequence(int len, float gc)

</class AdjustEntropy>
<class AllToAllVectorMaker>
*@author Brian Bushnell
*@date Feb 23, 2025
public class AllToAllVectorMaker

#Fields
FloatList[][] kmerDifMatrix=new FloatList[2][38]

private String out1=null

private String outKmerDif=null

private String outKmerFraction=null

private final FileFormat ffout1

DataLoader loader=null

long seed=-1

long lines=1000000

long linesOut=0

long posCount=0

long negCount=0

float positiveRate=0.5f

float edgeFraction=0.1f

int baseRolls=2

long positiveLines=0

long negativeLines=0

int maxClusterContigs=9

Random randy

double dif3good=0

double dif34good=0

long count3good=0

double dif45good=0

double dif5good=0

long count5good=0

float maxGCDif=1.0f

float maxDepthRatio=1000.0f

float maxKmerDif=1.0f

final float maxProduct

int minlen=100

int maxlen=2000000000

ArrayList<Contig> allContigs=null

ArrayList<ArrayList<Contig>> allSets=null

private final ByteBuilder lineBuffer=new ByteBuilder()

private final FloatList vecBuffer=new FloatList()

private boolean errorState=false

private java.io.PrintStream outstream=System.err

public static boolean verbose=false


#Methods
public static void main(String[] args)

public AllToAllVectorMaker(String[] args)

void process(Timer t)

private void outputResults(ArrayList<Contig> contigs, HashMap<Integer,ArrayList<Contig>> map)

private void outputKmerDifs(String fname, int sign)

private void outputKmerDifFraction(String fname, double incr, double max)

private FloatList fractionIndexedByKmerDif(FloatList plus, FloatList minus, double incr, double max)

private ByteBuilder makeLine(ArrayList<ArrayList<Contig>> clusters, Oracle oracle, boolean positive)

private FloatList makeVector(ArrayList<Contig> alist, ArrayList<Contig> blist, int minSize, int maxSize, Oracle oracle)

private void decluster(Cluster clust)

private void trackRatio(Bin a, Bin b)

private Bin selectBin(ArrayList<Contig> list, int maxElements, int minSize, int maxSize, IntHashSet used)

private Cluster selectCluster(ArrayList<Contig> list, int maxElements, int minSize, int maxSize, IntHashSet used, int tries)

private Contig selectContig(ArrayList<Contig> list, int minSize, int maxSize, IntHashSet used)

private ByteBuilder toLine(FloatList vector)

private boolean passesFilter(Bin a, Bin b)

private int randomIndex(Random randy, int max, int rolls)

private static long toKey(int a, int b)

FloatList getDifList(int size, int sameGenome)

</class AllToAllVectorMaker>
<class Bin>
public abstract class Bin

#Fields
public int numTetramers

public int numPentamers

public int[] dimers

public int[] trimers

public int[] tetramers

public int[] pentamers

public long gcSum

public long sketchedSize

public long depthSum=0

private float avgDepth=-1

boolean avgDepthValid=false

private FloatList depth=new FloatList(1)

private float[] normDepth

public IntHashMap pairMap

float completeness=0

float contam=0

int badContigs=0

float entropy

float strandedness

float score

public boolean wasReclustered=false

int dest=-1

public int taxid

public int genusTaxid

public int labelTaxid

SketchRecord topHit

SketchRecord secondHit

public byte[] r16S

public byte[] r18S


#Methods
@Override public final int taxid()

String name()

@Override public final float gc()

public final void clearDepth()

public float depthRatio(Bin b, int method)

public float depthRatio(Bin b)

public float depthRatio2(Bin b)

public float depthRatio3(Bin b)

public float depthRatio4(Bin b)

public boolean pure()

public boolean pure(float fraction)

public final void setDepth(float d, int sample)

public final void appendDepth(float d)

public final int numDepths()

public final float numEdges()

public final float depth(int sample)

public float minContigDepth()

public float maxContigDepth()

public final float maxDepth()

public final float depthTotal()

public float[] normDepth()

void fillNormDepth()

*Uses a weighted sum of linear and geometric means
public final float depth()

@Override public final int compareTo(Sketchable o)

@Override public final void setFrom(JsonObject all)

@Override public final void clearTax()

@Override public final String toString()

public final ByteBuilder toBytes()

*Higher is more similar
public float covariance(Bin b)

public final long sketchedSize()

public final void calcContam(IntLongHashMap sizeMap)

public final void calcContamContigs(IntLongHashMap sizeMap)

public final int primaryTaxid()

boolean sameCluster(Bin b)

public boolean isCluster()

public Cluster toCluster()

public Cluster cluster()

public boolean isValid()

public final boolean isEmpty()

public int transEdgesTo(Bin b)

public int transEdgesTo(Contig b)

public int transEdgesTo(Cluster b)

public int countEdgesTo(Bin b)

public int countEdgesTo(Contig b)

public int countEdgesTo(Cluster b)

public int countReciprocalEdges(Bin b)

public int countReciprocalEdges(Contig b)

public int countReciprocalEdges(Cluster b)

public FloatList depthList()

final boolean hasSSU()

</class Bin>
<class BinComparator>
class BinComparator

#Methods
@Override public int compare(Bin a, Bin b)

</class BinComparator>
<class BinMap>
public class BinMap

#Fields
public ConcurrentHashMap<Key,ArrayList<Cluster>> map=new ConcurrentHashMap<Key,ArrayList<Cluster>>(2000,0.6f,32)

public ArrayList<Bin> residual=new ArrayList<Bin>()

public ArrayList<Contig> contigList

private int minGridGC=999999

private int maxGridGC=0

private int minGridDepth=999999

private int maxGridDepth=0


#Methods
public BinMap(ArrayList<Contig> contigs)

*Higher is less stringent; 1.0 is neutral, 0 is exact match
public Cluster addOrMerge(Bin a, int minSizeToCompare, int minSizeToMerge, int minSizeToAdd, Oracle oracle, Key key, int matrixRange)

void addAll(Collection<? extends Bin> bins, int minSize)

Cluster add(Bin a, Key key)

public ArrayList<Cluster> getOrMakeList(Key key)

public Cluster findBestCluster(Bin a, int minSizeToCompare, Key key, int matrixRange, Oracle oracle)

private int findBestBinIndex(Bin a, ArrayList<? extends Bin> clusters, int minSizeToCompare, Oracle oracle)

ArrayList<Cluster> toList(boolean addResidue)

@Override public Iterator<Cluster> iterator()

public void clear(boolean clearResidual)

public boolean isValid()

public int countClusters()

</class BinMap>
<class Binner>
public class Binner

#Fields
boolean multiThreadedCompare=true

BinSketcher sketcher

int baseRange=1

int basePasses=1

int residueRange=3

boolean errorState=false

boolean runPassAA=false

boolean runPassA=true

boolean runPassB=false

boolean runPassC=true

boolean runPassD=true

boolean runPassE=true

boolean runPassF=false

boolean runPassG=true

long goodMergesFollow=0

long badMergesFollow=0

long goodMergeSizeFollow=0

long badMergeSizeFollow=0

long goodMergesCreate=0

long badMergesCreate=0

long goodMergeSizeCreate=0

long badMergeSizeCreate=0

long goodMergesRefine=0

long badMergesRefine=0

long goodMergeSizeRefine=0

long badMergeSizeRefine=0

long goodMergesResidue=0

long badMergesResidue=0

long goodMergeSizeResidue=0

long badMergeSizeResidue=0

long[] goodMergeSize=new long[40]

long[] badMergeSize=new long[40]

BinMap binMap

private int threadMerges=0

public AtomicLong fastComparisons=new AtomicLong(0)

public AtomicLong trimerComparisons=new AtomicLong(0)

public AtomicLong tetramerComparisons=new AtomicLong(0)

public AtomicLong slowComparisons=new AtomicLong(0)

public AtomicLong netComparisons=new AtomicLong(0)

final Timer phaseTimer=new Timer()

final PrintStream outstream

static boolean PERFECT_ORACLE=false

static boolean BAN_BAD_MERGES=false

static final int REFINE_MODE=0

static final int RESIDUE_MODE=1

static final int PURIFY_MODE=2

static final int FOLLOW_MODE=3

static final int FUSE_MODE=4

static final int RECLUSTER_MODE=5

static int fuseLowerLimit=5000

static int fuseUpperLimit=900000

static float fuseStringency=1.5f

static float purifyStringency=3f

static float residueStringency=0.65f

static float productMult=0.68f

static float minSimilarity=0.0f

static int maxEdges=2

static int minEdgeWeight=2

static boolean reciprocalEdges=true

static float minEdgeRatio=0.4f

static float lowDepthEdgeRatio=0.2f

static float highDepthEdgeRatio=2f

static float goodEdgeMult=1.35f

static float goodTransEdgeMult=1.25f

static int hugeThresh=1200000

static float hugeMult=0.375f

static int bigThresh=100000

static float bigMult=0.725f

static int smallThresh=8000

static float smallMult=2.0f

static int tinyThresh=1000

static float tinyMult=0.72f

*Size of the bigger one
static int minSizeToCompare=2500

*Size of the smaller one being compared
static int minSizeToMerge=2500

static int minSizeResidue=200

static int minNetSize=200

static int midNetSize=3000

static int largeNetSize=15000

static float netCutoffUpper=0.65f

static float netCutoffLower=0.547f

static float netCutoff_small=0.52f

static float netCutoff_mid=0.52f

static float netCutoff_large=0.52f

static float netCutoff1=0.65f

static float netMultUpper=1.4f

static float netMultLower=0.5f

static float cutoffMultA=2.7f

static float cutoffMultB=1.7f

static float cutoffMultC=1.6f

static float cutoffMultD=1.2f

static int overrideSetSamples=-1

static int compareThreadsOverride=-1

static float max3merDif1=0.1f

static float max4merDif1=0.002f

static float max5merDif1=0.003f

static float maxDepthRatio1=1.05f

static float maxGCDif1=0.015f

static float maxCovariance1=0.0001f

static float max3merDif2=0.1f

static float max4merDif2=0.005f

static float max5merDif2=0.007f

static float maxDepthRatio2=1.36f

static float maxGCDif2=0.03f

static float maxCovariance2=0.004f

static float minKmerProb2=0.9f


#Methods
Binner(PrintStream outstream_)

boolean parse(String arg, String a, String b)

public long followEdges(ArrayList<Contig> contigs, ArrayList<? extends Bin> input, Oracle oracle)

private int mergeWithDest(ArrayList<Contig> contigs, ArrayList<? extends Bin> input)

private static final int countClustered(ArrayList<? extends Bin> list)

public static final ArrayList<Bin> toBinList(ArrayList<? extends Bin> list, int minSize)

private int followEdges(Bin a, ArrayList<Contig> contigs, Oracle oracle)

public Cluster findLinkedCluster(Bin a, ArrayList<Contig> contigs)

public BinMap makeBinMap(ArrayList<Contig> contigList, ArrayList<? extends Bin> input)

public int refineBinMapPass(BinMap map, float stringency, int taxlevel, boolean allowNoTaxID, boolean allowHalfTaxID, int range, int minSize)

public int recluster(BinMap map, float stringency, int minSizeRecluster)

public int purify(BinMap map, float stringency, int range, int minSizePurify, int minSizeCompare)

int purifyCluster(Cluster clust)

public int processResidue(BinMap map, float stringency, int taxlevel, boolean allowNoTaxID, boolean allowHalfTaxID, int range)

public Cluster findBestResidualCluster(Bin a, BinMap map, Key key, Oracle oracle, int range, int minSize)

public int refineBinMap(BinMap map)

int refinePhase(BinMap map, String phase, float stringency, int taxLevel, boolean noTax, boolean halfTax, int range, int initialMinSize, int passes)

public ArrayList<Bin> clusterByTaxid(ArrayList<? extends Bin> bins)

public long fuse(ArrayList<Contig> contigs, ArrayList<? extends Bin> input, float stringency)

void setSamples(int samples, float mult)

void printThresholds()

private long launchThreads(ArrayList<? extends Bin> list, BinMap map, ArrayList<Contig> contigs, int mode, int range, int minSize, Oracle oracle)

@Override public void accumulate(CompareThread t)

@Override public ReadWriteLock rwlock()

@Override public boolean success()

static float sizeAdjustMult(long size)

static float sizeAdjustMult2(long size)

void addMerge(long size, boolean good)

</class Binner>
<class Binner.CompareThread>
class Binner.CompareThread

#Fields
final ArrayList<? extends Bin> input

final BinMap map

final ArrayList<Contig> contigs

final int tid

final int threads

final int mode

long fuseCompares=0

long fuseSeeks=0

long fuseTargets=0

final int range

final int minSize

final Key key=new Key()

final float[] ret=new float[1]

final Oracle oracle

boolean success=false

int mergesT=0


#Methods
public CompareThread(ArrayList<? extends Bin> list, BinMap map_, ArrayList<Contig> contigs_, int tid_, int threads_, int mode_, int range_, int minSize_, Oracle oracle_)

public void run()

private int follow()

private int fuse()

private Bin findBestFuseTarget(Bin a)

private int recluster()

private int purify()

private int purifyCluster(Cluster clust)

private void residue()

private void refine()

private void refineBin(Bin a)

</class Binner.CompareThread>
<class BinObject>
*Superclass for binner classes
*@author Brian Bushnell
*@date Feb 4, 2025
public class BinObject

#Fields
private static int quant=1

*Maps a kmer to index in frequency array
public static int[][] remapMatrix=makeRemapMatrix(2,5,true)

*Number of canonical kmers; frequency array length
public static int[] canonicalKmers=makeCanonicalKmers()

public static float[] invCanonicalKmers=makeInvCanonicalKmers()

*Maps a kmer to index in gc content array
public static int[][] gcmapMatrix=makeGCMapMatrix()

private static final int[] masks={0,3,15,63,255,1023,4095}

*Print status messages to this output stream
static PrintStream outstream=System.err

static TaxTree tree=null

static String treePath="auto"

static int minClusterSize=50000

static int minContigsPerCluster=1

static float depthBoost=0.25f

static int depthRatioMethod=4

static boolean addEuclidian=false

static boolean addHellinger=true

static boolean addHellinger3=true

static boolean addHellinger5=true

static boolean addAbsDif=true

static boolean addJsDiv=true

static boolean addEntropy=true

static boolean addStrandedness=true

static boolean addGCComp=true

static float vectorSmallNumberMult=5f

static boolean vectorSmallNumberRoot=false

static boolean makingBinMap=false

public static boolean countTrimers=true

public static boolean countPentamers=true

public static int minPentamerSizeCount=2000

public static int minPentamerSizeCompare=40000

static boolean loud=false

static boolean verbose

static boolean printStepwiseCC=false

static float sketchDensity=1 / 100f

static boolean sketchContigs=false

static boolean sketchClusters=false

static boolean sketchOutput=false

static boolean sketchInBulk=true

static int sketchSize=20000

static boolean validation=false

static boolean grading=false

static boolean parseTaxid=true

static boolean depthZeroProxy=true

static int globalTime=0

static double[] sampleDepthSum

static double[] invSampleDepthSum

static double sampleEntropy=1

static int samplesEquivalent=1

static CellNet net0small=null

static CellNet net0mid=null

static CellNet net0large=null

static int entropyK=4

static int entropyWindow=150

static boolean calcCladeEntropy=false

static int MIN_LINEAGE_LEVEL_E=0


#Methods
public static void setQuant(int x)

private static void initialize()

private static int[][] makeRemapMatrix(int mink, int maxk, boolean specialCase2)

private static int[][] makeGCMapMatrix()

private static int[] makeCanonicalKmers()

private static float[] makeInvCanonicalKmers()

public static int[] makeRemap(int k)

public static int ungap(int kmer, int k, int gap)

public static int[] gcmap(int k, int[] remap)

public static int countKmers(byte[] bases, int[] counts, int k)

public static void countKmersMulti(byte[] bases, long[][] counts, int kmax)

public static int countKmers_quantized(byte[] bases, int[] counts, int k)

*@param a Contig kmer frequencies
*@param b Cluster kmer frequencies
*@return Score
static final float absDif(float[] a, float[] b)

static final float absDif(int[] a, int[] b)

*@param a Contig kmer counts
*@param b Cluster kmer counts
*@return Score
static final float absDif(int[] a, int[] b, float inva, float invb)

*@param a Contig kmer frequencies
*@param b Cluster kmer frequencies
*@return Score
static final float rmsDif(float[] a, float[] b)

static final float rmsDif(int[] a, int[] b)

*@param a Contig kmer counts
*@param b Cluster kmer counts
*@return Score
static final float rmsDif(int[] a, int[] b, float inva, float invb)

*@param a Contig kmer frequencies
*@param b Cluster kmer frequencies
*@return Score
static final float ksFunction(float[] a, float[] b)

static boolean isValid(Collection<? extends Bin> list, boolean allowLeafContigs)

static float calculateShannonEntropy(FloatList depths, int limit)

static int calculateDistinctValues(FloatList depths)

static TaxTree loadTree()

*Looks for tid_1234 or tid|1234, with any delimiters
public static int parseTaxID(String line)

public static int resolveTaxID(String s)

</class BinObject>
<class BinSketcher>
*Handles sketches and taxonomic assignments for contigs and clusters.
*@author Brian Bushnell
*@date December 11, 2024
public class BinSketcher

#Fields
long linesProcessed=0

long linesOut=0

long bytesProcessed=0

long bytesOut=0

private final SketchTool tool

private final int threads

final int minSize

static int sectionSize=100

private final ReadWriteLock rwlock=new ReentrantReadWriteLock()

*Print status messages to this output stream
private PrintStream outstream=System.err

*Print verbose messages
public static boolean verbose=false

*True if an error was encountered
public boolean errorState=false


#Methods
public BinSketcher(int threads_, int minSize_)

void sketch(ArrayList<? extends Sketchable> input, boolean force)

*Spawn process threads
private void spawnThreads(ArrayList<? extends Sketchable> list)

@Override public final void accumulate(ProcessThread pt)

@Override public final boolean success()

@Override public final ReadWriteLock rwlock()

</class BinSketcher>
<class BinSketcher.ProcessThread>
*This class is static to prevent accidental writing to shared variables.
*It is safe to remove the static modifier.
class BinSketcher.ProcessThread

#Fields
final Read dummy=new Read(null,null,null,0)

final JsonParser jp=new JsonParser()

final DisplayParams params

protected boolean errorStateT=false

*True only if this thread has completed successfully
boolean success=false

*Input
private final ArrayList<? extends Sketchable> contigs

*Thread ID
final int tid

*Thread ID
final int threads

final SketchMakerMini smm


#Methods
ProcessThread(ArrayList<? extends Sketchable> contigs_, int tid_, int threads_)

@Override public void run()

*Iterate through the lines
void processInner_oneByOne()

*Iterate through the lines
void processInner_bulk()

*Iterate through the lines
void processSection(int from, int to)

</class BinSketcher.ProcessThread>
<class BinStats>
public class BinStats

#Fields
final String name

int id

int taxid

long size

int contigs

int badContigs

float contam

float complt

float gc

float depth

float minDepth

float maxDepth

int r5Scount=0

int r16Scount=0

int r18Scount=0

int r23Scount=0

int trnaCount=0

int cdsCount=0

long cdsLength=0

public String lineage


#Methods
BinStats(Bin b, String name_)

@Override public int compareTo(BinStats b)

String type(boolean useRNA)

public boolean hq(boolean useRNA)

public boolean mq(boolean useRNA)

static String type(float complt, float contam, int r16S, int r23S, int r5S, int trna, boolean useRNA)

</class BinStats>
<class BinStatsComparator>
class BinStatsComparator

#Methods
@Override public int compare(BinStats a, BinStats b)

</class BinStatsComparator>
<class ChartMaker>
public class ChartMaker

#Methods
static void makeChartFromBinStats(String fname, ArrayList<BinStats> list)

static void writeCCPlot(String fname, ArrayList<BinStats> list)

static void writeContamHist(String fname, ArrayList<BinStats> list)

</class ChartMaker>
<class Clade>
*Represents a taxonomic clade with k-mer frequency signatures.
*Contains 1-mer through 5-mer frequencies and various statistics for genome comparison.
*K-mers are stored as canonical forms to reduce dimensionality.
*@author Brian Bushnell
*@date April 12, 2025
public class Clade

#Fields
*Taxonomic ID number
public int taxID=-1

*Taxonomic level (e.g., species, genus)
public int level=-1

*Taxonomic name
public String name=null

*Taxonomic lineage
public String lineage=null

*K-mer count arrays - index 1 for 1-mers, 2 for 2-mers, etc.
public final long[][] counts

*Normalized trimer frequencies or GC-compensated values
public float[] trimers

*Normalized tetramer frequencies or GC-compensated values
public float[] tetramers

public byte[] r16S

public byte[] r18S

*Total number of bases in this Clade
public long bases

*Number of contigs or sequences in this Clade
public long contigs

*GC content (fraction of G+C bases)
public float gc

*Shannon entropy of sequence
public float entropy

*GC-compensated entropy
public float gcCompEntropy

*Measure of strand bias
public float strandedness

*Flag indicating whether this Clade has been completed with finish()
private boolean finished=false

public static final int DECIMAL=0

public static final int A48=1

public static int outputCoding=DECIMAL

public static int MAXK=5

public static boolean callSSU=false

public static boolean writeLineage=true


#Methods
*Constructs a Clade with the specified taxonomic information.
*Initializes k-mer count arrays from 1-mer through 5-mer.
*@param taxID_ The taxonomic ID number
*@param level_ The taxonomic level (e.g., species, genus)
*@param name_ The taxonomic name
public Clade(int taxID_, int level_, String name_)

*Creates a Clade from a taxonomic ID by looking up taxonomic information.
*@param tid The taxonomic ID to look up
*@return A new Clade with information from the taxonomy tree, or a minimal Clade if ID not found
public static Clade makeClade(int tid)

*Convert a list of byte arrays to a Clade object.
*@param list List of byte arrays containing clade data
*@param lp LineParser for parsing the data
*@return Clade object created from the data
public static Clade parseClade(ArrayList<byte[]> list, LineParser1 lp)

*Adds a sequence to this Clade, updating k-mer counts and statistics.
*@param seq The sequence to add
*@param et An EntropyTracker for calculating sequence entropy
*@param caller A GeneCaller for calling 16S/18S
public void add(Read r, EntropyTracker et, GeneCaller caller)

*Adds a sequence to this Clade, updating k-mer counts and statistics.
*@param seq The sequence to add
*@param et An EntropyTracker for calculating sequence entropy
public void add(byte[] seq, EntropyTracker et)

*Merges another Clade into this one, combining counts and updating statistics.
*If this Clade is empty, it will adopt the taxonomic information of the other Clade.
*@param c The Clade to merge into this one
public void add(Clade c)

*Completes the Clade by calculating derived statistics.
*This includes GC content, strandedness, entropy compensation, and normalized k-mer distributions.
*Once completed, the Clade's state should not be modified.
public void finish()

*Calculates the GC content from the 1-mer counts.
*@return The fraction of G and C bases relative to all counted bases (A,C,G,T)
private float calcGC()

*Fills the normalized trimer (3-mer) frequency array.
*For ABSCOMP method, groups k-mers by GC content and normalizes within each group.
*This means CCC frequency would be calculated as (count of CCC)/(sum of counts for all 3-mers with 3 GC bases)
*rather than as a fraction of all 3-mers.
private void fillTrimers()

*Fills the normalized tetramer (4-mer) frequency array.
*For ABSCOMP method, groups k-mers by GC content and normalizes within each group.
*This means CCCC frequency would be calculated as (count of CCCC)/(sum of counts for all 4-mers with 4 GC bases)
*rather than as a fraction of all 4-mers.
private void fillTetramers()

boolean hasSSU()

*Resets the Clade to an empty state, clearing all counts and statistics.
public void clear()

*Compares Clades primarily by GC content, then by size, then by taxonomic ID.
*@param b The Clade to compare to
*@return Negative if this Clade should be ordered before b, positive if after
@Override public int compareTo(Clade b)

public CharSequence lineage()

*Gets the taxonomic lineage for a given taxonomic ID.
*@param tid Taxonomic ID
*@return Formatted taxonomic lineage string, or "NA" if unavailable
public static CharSequence lineage(int tid)

*Returns a simple string representation of this Clade.
*@return String containing taxonomic ID, GC content, and name
public String toString()

*Creates a detailed text representation of this Clade.
*Includes all taxonomic information, statistics, and k-mer counts.
*@param bb ByteBuilder to append to, or null to create a new one
*@return The ByteBuilder with appended Clade information
public ByteBuilder toBytes(ByteBuilder bb)

*Checks if this Clade has been completed with finish().
*@return true if finish() has been called, false otherwise
public boolean finished()

</class Clade>
<class CladeIndex>
*Indexes Clade objects by GC content for efficient similarity searching.
*Implements a GC-bucket based search strategy to quickly find the best matching reference
*for a query Clade using a two-stage comparison process.
*@author Brian Bushnell
*@date April 19, 2024
public class CladeIndex

#Fields
*Array of Clade lists indexed by GC percentage (0-100)
final ArrayList<Clade>[] gcDex=new ArrayList[101]

*Number of Clades loaded in this index
int cladesLoaded=0

*Total number of quick comparisons performed
long comparisons=0

*Number of detailed comparisons that passed the quick filter
long slowComparisons=0

*Number of intermediate comparisons to retain
static int heapSize=1

*Whether to exclude Clades with the same taxonomic ID from matches
static boolean banSelf=false

*Maximum number of GC buckets to search in each direction
static int maxSteps=7

*Maximum allowed GC difference (absolute)
static float gcDelta=0.07f

*Maximum allowed strandedness difference (absolute)
static float strDelta=0.10f

*Multiplier for dynamic GC difference threshold based on k-mer similarity
static float gcMult=0.5f

*Multiplier for dynamic strandedness difference threshold based on k-mer similarity
static float strMult=1.2f


#Methods
*Creates a new CladeIndex containing the specified Clades.
*Each Clade will be indexed by its GC content for fast retrieval.
*@param coll Collection of Clades to index
public CladeIndex(Collection<Clade> coll)

*Creates a clone of this CladeIndex.
*The clone shares the same indexed Clades but has zeroed comparison counters.
*@return A clone of this CladeIndex
public CladeIndex clone()

*Loads and indexes Clades from the specified reference files.
*@param ref Array of reference file paths
*@return A new CladeIndex containing Clades from the references
public static CladeIndex loadIndex(String ref)

*Loads and indexes Clades from the specified reference files.
*@param ref Collection of reference file paths
*@return A new CladeIndex containing Clades from the references
public static CladeIndex loadIndex(Collection<String> ref)

*Parses a command-line parameter and sets the corresponding configuration.
*Supports a wide range of tuning parameters for the search algorithm and comparison methods.
*@param arg Original argument string (unused)
*@param a Parameter name
*@param b Parameter value
*@return true if the parameter was recognized and processed, false otherwise
public static boolean parse(String arg, String a, String b)

*Adds a Clade to the index.
*The Clade is indexed by its GC content, rounded to the nearest percentage.
*@param c The Clade to add
public void add(Clade c)

*Finds the best match for a given Clade in the index.
*Uses a GC-bucket based search strategy that first checks Clades with similar GC content,
*then expands outward to neighboring GC buckets if needed.
*@param c The query Clade to find matches for
*@return An ordered list of Comparison objects containing the best matches found.
public ArrayList<Comparison> findBest(Clade c)

*Searches a specific GC bucket for the best match to the given Clade.
*Uses a two-stage comparison process with early filtering to improve performance.
*Updates the best match if a better one is found.
*@param a The query Clade to match
*@param list List of Clades in this GC bucket
*@param best Current best matches (will be updated if better matches are found)
*@param temp Temporary comparison object for reuse
*@param gcLevel The GC percentage (0-100) of this bucket
private void findBest(Clade a, ArrayList<Clade> list, ComparisonHeap heap, Comparison temp, int gcLevel)

*Gets the number of Clades indexed.
*@return Number of Clades in the index
public int size()

</class CladeIndex>
<class CladeLoader>
*Loads fasta files with TID-labeled contigs,
*to produce Clade record output with kmer frequencies, etc.
*@author Brian Bushnell
*@date April 12, 2025
public class CladeLoader

#Fields
*Primary input file path
private ArrayList<String> in=new ArrayList<String>()

private ArrayList<String> r16sFile=new ArrayList<String>()

private ArrayList<String> r18sFile=new ArrayList<String>()

int r16sAdded=0

int r18sAdded=0

*Primary output file path
private String out=null

*Override input file extension
private String extin=null

*Override output file extension
private String extout=null

*Number of reads processed
protected long readsProcessed=0

*Number of bases processed
protected long basesProcessed=0

*Number of lines out
protected long linesOut=0

*Number of bytes out
protected long bytesOut=0

*Quit after processing this many input reads; -1 means no limit
private long maxReads=-1

*Whether to use a dummy clade for temporary storage
static boolean useDummy=true

*Whether to merge duplicate tax IDs
static boolean mergeDuplicateTaxIDs=false

static boolean replaceRibo=false

static boolean useTree=false

*Primary output file
private FileFormat ffout

private final ReadWriteLock rwlock=new ReentrantReadWriteLock()

*Print status messages to this output stream
private PrintStream outstream=System.err

*Print verbose messages
public static boolean verbose=false

*True if an error was encountered
public boolean errorState=false

*Overwrite existing output files
private boolean overwrite=true

*Append to existing output files
private boolean append=false

*Reads are output in input order
private boolean ordered=false


#Methods
*Code entrance from the command line.
*@param args Command line arguments
public static void main(String[] args)

*Default constructor.
public CladeLoader()

*Constructor.
*@param args Command line arguments
public CladeLoader(String[] args)

*Parse arguments from the command line
private Parser parse(String[] args)

*Add or remove .gz or .bz2 as needed
private void fixExtensions()

*Ensure files can be read and written
private void checkFileExistence()

*Adjust file-related static fields as needed for this program
private static void checkStatics()

*Ensure parameter ranges are within bounds and required parameters are set
private boolean validateParams()

*Create read streams and process all data
void process(Timer t)

*Load clades from a collection of files.
*@param fnames Collection of file names to load
*@param map Map to store clades in (created if null)
*@return Map containing loaded clades
public ConcurrentHashMap<Integer,Clade> load(Collection<String> fnames, ConcurrentHashMap<Integer,Clade> map)

*Load clades from a single file.
*@param fname File name to load
*@param map Map to store clades in (created if null)
*@return Map containing loaded clades
public ConcurrentHashMap<Integer,Clade> load(String fname, ConcurrentHashMap<Integer,Clade> map)

*Load clades from sequences in a file.
*@param ff FileFormat for input
*@param cladeMap Map to store clades in (created if null)
*@return Map containing loaded clades
ConcurrentHashMap<Integer,Clade> loadFromSequence(FileFormat ff, ConcurrentHashMap<Integer,Clade> cladeMap)

*Load clades from a clade file.
*@param ff FileFormat for input
*@param cladeMap Map to store clades in (created if null)
*@return Map containing loaded clades
ConcurrentHashMap<Integer,Clade> loadFromClade(FileFormat ff, ConcurrentHashMap<Integer,Clade> cladeMap)

*Load clades from a ByteFile.
*@param bf ByteFile containing clade data
*@param map Map to store clades in
static void loadClades(ByteFile bf, ConcurrentHashMap<Integer,Clade> map)

private static boolean addClade(ArrayList<byte[]> set, ConcurrentHashMap<Integer,Clade> map, LineParser1 lp)

*Load clades from a clade-format file.
*@param ff FileFormat for input
*@return List of clades loaded from the file
public static ArrayList<Clade> loadCladesFromClade(FileFormat ff)

static int addRibo(ConcurrentHashMap<Integer,Clade> cladeMap, String ssuFile, boolean r16s)

static int addRibo(ConcurrentHashMap<Integer,Clade> map, ConcurrentReadInputStream cris, boolean r16s)

static int addRibo(ConcurrentHashMap<Integer,Clade> map, Read r, boolean r16s)

*Write a collection of clades to a file.
*@param ff FileFormat for output
*@param coll Collection of clades to write
void write(FileFormat ff, Collection<Clade> coll)

*Create a ConcurrentReadInputStream for a FileFormat.
*@param ff FileFormat to create stream from
*@return ConcurrentReadInputStream for the FileFormat
private ConcurrentReadInputStream makeCris(FileFormat ff)

*Spawn process threads
private void spawnThreads(ConcurrentReadInputStream cris, ConcurrentHashMap<Integer,Clade> cladeMap)

@Override public final void accumulate(ProcessThread pt)

@Override public final boolean success()

@Override public final ReadWriteLock rwlock()

</class CladeLoader>
<class CladeLoader.ProcessThread>
*This class is static to prevent accidental writing to shared variables.
*It is safe to remove the static modifier.
static class CladeLoader.ProcessThread

#Fields
*Number of reads processed by this thread
protected long readsProcessedT=0

*Number of bases processed by this thread
protected long basesProcessedT=0

*True only if this thread has completed successfully
boolean success=false

private EntropyTracker et=new EntropyTracker(entropyK,entropyWindow,false)

private final Clade dummy=new Clade(-1,-1,null)

*Shared input stream
private final ConcurrentReadInputStream cris

*Clade storage
private final ConcurrentHashMap<Integer,Clade> cladeMap

*Thread ID
final int tid


#Methods
ProcessThread(ConcurrentReadInputStream cris_, ConcurrentHashMap<Integer,Clade> cladeMap_, int tid_)

@Override public void run()

*Iterate through the reads
void processInner()

*Process a list of reads.
*@param ln ListNum containing reads to process
void processList(ListNum<Read> ln)

*Process a single contig.
*@param r Read containing contig data
*@return True if successfully processed
boolean processContig(Read r)

*Process a single contig using the dummy approach.
*@param r Read containing contig data
*@return True if successfully processed
boolean processContigDummy(Read r)

*Emit the current dummy clade to the main clade map.
private void emitDummy()

*Get a clade from the map or create a new one if it doesn't exist.
*@param tid Tax ID for the clade
*@return Clade object
private Clade getOrMakeClade(int tid)

</class CladeLoader.ProcessThread>
<class CladeLoaderMF>
*Designed to load one clade per file in metagenomic bins.
*Unlike CladeLoader, this class is optimized for multiple
*files with one taxonomic group per file
*(rather than a single file with multiple taxonomic IDs).
*Uses one thread per file for parallel processing.
*@author Brian Bushnell
*@date April 12, 2025
public class CladeLoaderMF

#Fields
*Primary input file path
private ArrayList<String> in=new ArrayList<String>()

*Primary output file path
private String out=null

*Override input file extension
private String extin=null

*Override output file extension
private String extout=null

*Whether to treat each contig as a separate clade
boolean perContig=false

*Minimum contig length to process
int minContig=0

*Number of reads processed
protected long readsProcessed=0

*Number of bases processed
protected long basesProcessed=0

*Number of lines out
protected long linesOut=0

*Number of bytes out
protected long bytesOut=0

*Quit after processing this many input reads; -1 means no limit
private long maxReads=-1

*Primary output file
private FileFormat ffout

private final ReadWriteLock rwlock=new ReentrantReadWriteLock()

*Print status messages to this output stream
private PrintStream outstream=System.err

*Print verbose messages
public static boolean verbose=false

*True if an error was encountered
public static boolean errorState=false

*Overwrite existing output files
private boolean overwrite=true

*Append to existing output files
private boolean append=false

*Reads are output in input order
private boolean ordered=false


#Methods
*Code entrance from the command line.
*@param args Command line arguments
public static void main(String[] args)

*Default constructor.
*Loads entropy model if needed.
public CladeLoaderMF()

*Constructor.
*@param args Command line arguments
public CladeLoaderMF(String[] args)

*Parse arguments from the command line
*@param args Arguments to parse
*@return Parser with parsed values
private Parser parse(String[] args)

*Ensure files can be read and written
*@throws RuntimeException If files cannot be read or written
private void checkFileExistence()

*Adjust file-related static fields as needed for this program
private static void checkStatics()

*Ensure parameter ranges are within bounds and required parameters are set
*@return True if parameters are valid
private boolean validateParams()

*Create read streams and process all data
*@param t Timer for tracking execution time
void process(Timer t)

*Write a collection of clades to a file.
*@param ff FileFormat for output
*@param coll Collection of clades to write
void write(FileFormat ff, Collection<Clade> coll)

*Load a single clade from a file.
*@param fname File name to load
*@param et Entropy tracker
*@return Clade loaded from the file
public static Clade loadOneClade(String fname, EntropyTracker et)

*Load a single clade from a clade-format file.
*@param ff FileFormat for input
*@return Clade loaded from the file
public static Clade loadOneCladeFromClade(FileFormat ff)

*Load a single clade from a sequence file.
*@param ff FileFormat for input
*@param et Entropy tracker
*@return Clade loaded from the file
private static Clade loadOneCladeFromSequence(FileFormat ff, EntropyTracker et)

*Load clades from a file.
*@param fname File name to load
*@param et Entropy tracker
*@param perContig Whether to treat each contig as a separate clade
*@param minContig Minimum contig length to process
*@return List of clades loaded from the file
public static ArrayList<Clade> loadClades(String fname, EntropyTracker et, boolean perContig, int minContig)

*Load clades from a clade-format file.
*@param ff FileFormat for input
*@return List of clades loaded from the file
public static ArrayList<Clade> loadCladesFromClade(FileFormat ff)

private static boolean addClade(ArrayList<byte[]> set, ArrayList<Clade> clades, LineParser1 lp)

*Create a ConcurrentReadInputStream for a FileFormat.
*@param ff FileFormat to create stream from
*@return ConcurrentReadInputStream for the FileFormat
private static ConcurrentReadInputStream makeCris(FileFormat ff)

*Load clades from a sequence file.
*@param ff FileFormat for input
*@param et Entropy tracker
*@param minContig Minimum contig length to process
*@return List of clades loaded from the file
private static ArrayList<Clade> loadCladesFromSequence(FileFormat ff, EntropyTracker et, int minContig)

*Process reads from a stream into clades.
*@param cris Input read stream
*@param et Entropy tracker
*@param minContig Minimum contig length to process
*@return List of clades processed from the reads
private static ArrayList<Clade> processReads(ConcurrentReadInputStream cris, EntropyTracker et, int minContig)

*Load clades from multiple files.
*@param in List of file paths to load
*@param perContig Whether to treat each contig as a separate clade
*@param minContig Minimum contig length to process
*@return List of all clades loaded from the files
public ArrayList<Clade> loadFiles(ArrayList<String> in, boolean perContig, int minContig)

*Spawn process threads to handle multiple files in parallel.
*@param files List of file paths to process
*@param perContig Whether to treat each contig as a separate clade
*@param minContig Minimum contig length to process
*@return List of all clades loaded from the files
private ArrayList<Clade> spawnThreads(ArrayList<String> files, boolean perContig, int minContig)

*Accumulate statistics from a ProcessThread.
*@param pt ProcessThread to accumulate from
@Override public final void accumulate(ProcessThread pt)

*Check if processing was successful.
*@return True if no errors occurred
@Override public final boolean success()

@Override public final ReadWriteLock rwlock()

</class CladeLoaderMF>
<class CladeLoaderMF.ProcessThread>
*Thread for processing files in parallel.
*This class is static to prevent accidental writing to shared variables.
*It is safe to remove the static modifier.
static class CladeLoaderMF.ProcessThread

#Fields
*Number of reads processed by this thread
protected long readsProcessedT=0

*Number of bases processed by this thread
protected long basesProcessedT=0

*True only if this thread has completed successfully
boolean success=false

private EntropyTracker et=new EntropyTracker(entropyK,entropyWindow,false)

private final boolean perContig

private final int minContig

*Shared data source
private final ArrayList<String> files

*Clade storage
private final ConcurrentHashMap<Integer,ArrayList<Clade>> cladeMap

*Thread ID
final int tid

*Total number of threads
final int threads


#Methods
*Constructor.
*@param files_ List of files to process
*@param cladeMap_ Map to store processed clades
*@param tid_ Thread ID
*@param threads_ Total number of threads
*@param perContig_ Whether to treat each contig as a separate clade
*@param minContig_ Minimum contig length to process
ProcessThread(ArrayList<String> files_, ConcurrentHashMap<Integer,ArrayList<Clade>> cladeMap_, int tid_, int threads_, boolean perContig_, int minContig_)

*Process assigned files.
*Called by start()
@Override public void run()

</class CladeLoaderMF.ProcessThread>
<class CladeSearcher>
*Command-line tool for searching genomic sequences against a reference database.
*Identifies the closest taxonomic match for query genomes using k-mer profiling.
*Supports multithreaded processing for efficient searching of large databases.
*@author Brian Bushnell
*@date April 12, 2025
public class CladeSearcher

#Fields
*Primary input file path
private ArrayList<String> in=new ArrayList<String>()

*Primary output file path
private String out="stdout.txt"

*Reference database paths
private ArrayList<String> ref=new ArrayList<String>()

*Loaded query clades
private ArrayList<Clade> queries

*Override input file extension
private String extin=null

*Override output file extension
private String extout=null

*The clade index for reference searching
public CladeIndex index

*Whether to print detailed comparison metrics
boolean printMetrics=false

*Whether to use multiple threads for searching
boolean multithreaded=true

*Whether to process each contig separately
boolean perContig=false

*Minimum contig length to process
int minContig=0

*Maximum number of threads to use for comparison
int maxCompareThreads=9999

*Whether to print query taxon IDs in output
boolean printQTID=false

*Output format (HUMAN or MACHINE)
int format=1

*Format constants
public static final int HUMAN=1

*Format constants
public static final int MACHINE=2

*Number of reads processed
protected long readsProcessed=0

*Number of bases processed
protected long basesProcessed=0

*Number of lines out
protected long linesOut=0

*Number of bytes out
protected long bytesOut=0

*Quit after processing this many input reads; -1 means no limit
private long maxReads=-1

private int maxHitsToPrint=1

*Primary output file
private FileFormat ffout

private final ReadWriteLock rwlock=new ReentrantReadWriteLock()

*Whether to use taxonomy tree for evaluation
static boolean useTree=false

*Print status messages to this output stream
private PrintStream outstream=System.err

*Print verbose messages
public static boolean verbose=false

*True if an error was encountered
public static boolean errorState=false

*Overwrite existing output files
private boolean overwrite=true

*Append to existing output files
private boolean append=false

*Reads are output in input order
private boolean ordered=false


#Methods
*Code entrance from the command line.
*@param args Command line arguments
public static void main(String[] args)

*Default constructor.
*Ensures the entropy adjustment model is loaded.
public CladeSearcher()

*Constructor with command line arguments.
*Parses arguments, sets up parameters, and validates input/output paths.
*@param args Command line arguments
public CladeSearcher(String[] args)

*Returns the default reference database path based on the execution environment.
*@return Path to the default reference database, or null if unavailable
public static String defaultRef()

*Sets up the searcher by loading necessary components.
*Loads entropy model and taxonomy tree if needed.
void setup()

*Loads and indexes reference clades from the specified files.
void loadIndex()

*Loads query clades from input files.
*Can process individual contigs separately if perContig is true.
void loadQueries()

*Parses command line arguments.
*Handles program-specific parameters and delegates standard flags to Parser.
*@param args Command line arguments
*@return Parser with parsed standard flags
private Parser parse(String[] args)

*Ensures input files can be read and output files can be written.
*Throws an exception if file access issues are detected.
private void checkFileExistence()

*Adjusts file-related static fields as needed for this program.
*Optimizes file reading mode based on thread count.
private static void checkStatics()

*Ensures parameter ranges are within bounds and required parameters are set.
*@return true if parameters are valid
private boolean validateParams()

*Main processing method.
*Creates read streams, processes all data, and writes results.
*@param t0 Timer for overall execution time tracking
void process(Timer t0)

*Performs single-threaded search of queries against the index.
*@param queries List of query Clades to search
*@param index The CladeIndex to search against
*@return List of Comparison results in the same order as queries
public ArrayList<Object> searchST(ArrayList<Clade> queries, CladeIndex index)

*Evaluates search results and prints performance metrics.
*Calculates correctness statistics and taxonomic level accuracy.
*@param results List of Comparison results to evaluate
public void evaluate(ArrayList<Object> results)

*Writes comparison results to the specified output format.
*@param ff Output file format
*@param comps Collection of comparison results to write
void write(FileFormat ff, Collection<Object> comps)

ByteBuilder appendResult(Object o, ByteBuilder bb)

*Appends a comparison result to a ByteBuilder in the configured format.
*@param c Comparison result to append
*@param bb ByteBuilder to append to
*@return The ByteBuilder with appended result
ByteBuilder appendResult(Comparison c, ByteBuilder bb, int hitNum)

*Appends a comparison result in human-readable format.
*@param c Comparison result to append
*@param bb ByteBuilder to append to
*@return The ByteBuilder with appended result
ByteBuilder appendResultHuman(Comparison c, ByteBuilder bb, int hitNum)

*Appends a comparison result in machine-readable format.
*@param c Comparison result to append
*@param bb ByteBuilder to append to
*@return The ByteBuilder with appended result
ByteBuilder appendResultMachine(Comparison c, ByteBuilder bb)

*Spawns multiple threads to process queries in parallel.
*@param queries List of query Clades to process
*@param index The CladeIndex to search against
*@return List of Comparison results in the same order as queries
private ArrayList<Object> spawnThreads(ArrayList<Clade> queries, CladeIndex index)

*Accumulates statistics from a finished ProcessThread.
*Implementation of Accumulator interface.
*@param pt ProcessThread to accumulate statistics from
@Override public final void accumulate(ProcessThread pt)

*Returns whether all processing was successful.
*Implementation of Accumulator interface.
*@return true if no errors occurred during processing
@Override public final boolean success()

@Override public final ReadWriteLock rwlock()

</class CladeSearcher>
<class CladeSearcher.ProcessThread>
*Worker thread for parallel clade search processing.
*Each thread processes a subset of queries assigned by thread ID.
static class CladeSearcher.ProcessThread

#Fields
*Number of reads processed by this thread
protected long readsProcessedT=0

*Number of bases processed by this thread
protected long basesProcessedT=0

*True only if this thread has completed successfully
boolean success=false

final CladeIndex index

*Shared data source
private final ArrayList<Clade> queries

*Clade storage
private final ArrayList<Object> results=new ArrayList<Object>()

*Thread ID
final int tid

final int threads


#Methods
*Constructs a new ProcessThread.
*@param queries_ List of query Clades to process
*@param index_ The CladeIndex to search against
*@param tid_ Thread ID
*@param threads_ Total number of threads
ProcessThread(ArrayList<Clade> queries_, CladeIndex index_, int tid_, int threads_)

*Main thread execution method.
*Processes queries assigned to this thread and stores results.
@Override public void run()

</class CladeSearcher.ProcessThread>
<class Cluster>
public class Cluster

#Fields
final int id

public long size

public ArrayList<Contig> contigs=new ArrayList<Contig>(8)

public IntHashSet contigSet=new IntHashSet(5)

private int timestamp=-1

long nonDominantSize=0


#Methods
public Cluster(int id_)

public Cluster(Contig c)

@Override public String name()

@Override public boolean isCluster()

@Override public Cluster toCluster()

@Override public Cluster cluster()

@Override public final int id()

public boolean pure(float fraction)

public float minContigDepth()

public float maxContigDepth()

@Override @Deprecated public void setID(int id_)

public Cluster add(Bin b)

public Cluster add(Cluster clust)

boolean sameCluster(Bin b)

public Cluster add(Contig c)

@Override public long size()

@Override public Sketch toSketch(SketchMakerMini smm, Read r)

@Override public boolean isValid()

public void clear()

@Override public int numContigs()

@Override public Iterator<Contig> iterator()

</class Cluster>
<class Comparison>
*Compares two Clade objects to determine their genomic similarity.
*Implements multiple comparison methods including absolute difference,
*cosine similarity, Hellinger distance, Euclidean distance, and
*GC-compensated absolute difference.
*@author Brian Bushnell
*@date April 19, 2024
public class Comparison

#Fields
*The query Clade being compared
Clade query

*The query Clade being compared
Clade ref

*GC content difference between query and reference
float gcdif=1

*Entropy difference between query and reference
float entdif=1

*Strandedness difference between query and reference
float strdif=1

*3-mer profile difference between query and reference
float k3dif=1

*4-mer profile difference between query and reference
float k4dif=1

*5-mer profile difference between query and reference
float k5dif=1

float ssudif=1

*Whether to use early exit optimization to avoid unnecessary calculations
static boolean earlyExit=true

*Multiplier for 4-mer cutoff in early exit tests
private static float comparisonCutoffMult=1f

*Multiplier for 3-mer cutoff in early exit tests; 1.0 is better for ABS, 1.5 for ABSCOMP
private static float comparisonCutoffMult2=1.5f

*Constants for different comparison methods
static final int ABS=1

*Constants for different comparison methods
static final int COS=2

*Constants for different comparison methods
static final int HEL=3

*Constants for different comparison methods
static final int EUC=4

*Constants for different comparison methods
static final int ABSCOMP=5

*The current comparison method (default is ABSCOMP)
static int method=ABSCOMP

*Maximum k-mer size to use in comparisons
static int maxK=5


#Methods
*Creates an empty Comparison object.
public Comparison()

*Creates a Comparison between query and reference Clades using default parameters.
*@param query_ The query Clade
*@param ref_ The reference Clade
public Comparison(Clade query_, Clade ref_)

*Compares two Clades using a two-stage process.
*First performs a quick filtering based on GC content and strandedness,
*then performs a more detailed k-mer based comparison if the quick filter passes.
*@param q The query Clade
*@param r The reference Clade
*@param gcLimit Maximum allowed GC content difference
*@param strLimit Maximum allowed strandedness difference
*@param k5Limit Maximum allowed 5-mer profile difference
*@return The calculated similarity measure (lower is more similar)
float compare(Clade q, Clade r, float gcLimit, float strLimit, float k5Limit)

*Performs a rapid preliminary comparison based on GC content and strandedness.
*Sets the query and ref fields and calculates basic difference metrics.
*@param q The query Clade
*@param r The reference Clade
*@param gcLimit Maximum allowed GC content difference
*@param strLimit Maximum allowed strandedness difference
*@return true if the Clades pass the quick comparison filter
boolean quickCompare(Clade q, Clade r, float gcLimit, float strLimit)

*Performs a detailed comparison using k-mer frequency profiles.
*The specific comparison method is determined by the static method field.
*@param q The query Clade
*@param r The reference Clade
*@param k5Limit Maximum allowed 5-mer profile difference
*@return The calculated similarity measure (lower is more similar)
float slowCompare(Clade q, Clade r, float k5Limit)

*Compares Clades using absolute difference between k-mer frequencies.
*Uses early exit optimizations to avoid unnecessary calculations when possible.
*@param k5Limit Maximum allowed 5-mer profile difference
*@return The calculated absolute difference (lower is more similar)
private float compareABS(float k5Limit)

*Compares Clades using GC-compensated absolute difference between k-mer frequencies.
*K-mers are grouped by GC content before comparison to reduce GC bias.
*Uses early exit optimizations to avoid unnecessary calculations when possible.
*@param k5Limit Maximum allowed 5-mer profile difference
*@return The calculated GC-compensated absolute difference (lower is more similar)
private float compareABSCOMP(float k5Limit)

*Compares Clades using cosine distance between k-mer frequency vectors.
*Uses early exit optimizations to avoid unnecessary calculations when possible.
*@param k5Limit Maximum allowed 5-mer profile difference
*@return The calculated cosine distance (lower is more similar)
private float compareCOS(float k5Limit)

*Compares Clades using Hellinger distance between k-mer frequency distributions.
*Uses early exit optimizations to avoid unnecessary calculations when possible.
*@param k5Limit Maximum allowed 5-mer profile difference
*@return The calculated Hellinger distance (lower is more similar)
private float compareHEL(float k5Limit)

*Compares Clades using Euclidean distance between k-mer frequency vectors.
*Uses early exit optimizations to avoid unnecessary calculations when possible.
*@param k5Limit Maximum allowed 5-mer profile difference
*@return The calculated Euclidean distance (lower is more similar)
private float compareEUC(float k5Limit)

*Resets all difference measures to their default values.
void clearDif()

*Copies all values from another Comparison object to this one.
*@param b The source Comparison to copy from
public void setFrom(Comparison b)

public final boolean align(IDAligner ssa)

*Returns a string representation of this Comparison.
*@return A descriptive string, or null if the reference Clade is null
public String toString()

*Creates a detailed text representation of this Comparison.
*Includes reference taxon ID, name, and all difference measures.
*If a taxonomy tree is available, also includes taxonomic information.
*@param bb ByteBuilder to append to, or null to create a new one
*@return The ByteBuilder with appended Comparison information
public ByteBuilder toBytes(ByteBuilder bb)

*Appends human-readable comparison results to a ByteBuilder.
*Includes both query and top hit information.
*@param bb ByteBuilder to append to, or null to create a new one
*@return The ByteBuilder with appended human-readable results
ByteBuilder appendResultHuman(ByteBuilder bb, int hitNum)

*Creates a header line for machine-readable output format.
*@param printQTID Whether to include the query taxon ID column
*@return ByteBuilder containing the header line
public static ByteBuilder machineHeader(boolean printQTID)

*Appends machine-readable comparison results to a ByteBuilder.
*Format matches the header created by machineHeader().
*@param printQTID Whether to include the query taxon ID
*@param bb ByteBuilder to append to, or null to create a new one
*@return The ByteBuilder with appended machine-readable results
ByteBuilder appendResultMachine(boolean printQTID, ByteBuilder bb)

*Gets the taxonomic lineage of the reference Clade.
*@return Formatted taxonomic lineage string
CharSequence lineage()

*Calculates a weighted composite measure for comparison ranking.
*Gives highest weight to 5-mer differences, with smaller weights for other measures.
*@return The weighted comparison value (lower is more similar)
float pivot()

*Compares two Comparison objects to determine relative ordering.
*Uses a multi-tier comparison based on k-mer, GC, and other differences.
*@param b The Comparison to compare against
*@return -1 if this is more similar than b, 1 if less similar, 0 if equal
@Override public int compareTo(Comparison b)

*Determines if this comparison represents a correct classification.
*A correct classification is when query and reference have the same taxonomic ID.
*@return true if both query and reference have the same non-zero taxonomic ID
boolean correct()

*Determines if this comparison represents an incorrect classification.
*An incorrect classification is when query and reference have different taxonomic IDs.
*@return true if both query and reference have different non-zero taxonomic IDs
boolean incorrect()

*Finds the taxonomic level of the lowest common ancestor between query and reference.
*Higher return values indicate more distant relationships.
*@return Taxonomic level of the common ancestor, or TaxTree.LIFE if not determinable
int correctLevel()

*Gets the current comparison cutoff multiplier value.
*@return The current comparisonCutoffMult value
public static float ccm()

*Sets the comparison cutoff multiplier used in early exit optimizations.
*@param f The new multiplier value (must be non-negative)
public static void setComparisonCutoffMult(float f)

*Sets the secondary comparison cutoff multiplier used in early exit optimizations.
*@param f The new multiplier value (must be non-negative)
public static void setComparisonCutoffMult2(float f)

</class Comparison>
<class ComparisonHeap>
*Maintains a heap of the top N Comparisons encountered.
*Useful for finding the best N matches rather than just the single best match.
*@author Brian Bushnell
*@contributor Isla (Highly-customized Claude instance)
*@date April 19, 2024
public class ComparisonHeap

#Fields
private final PriorityQueue<Comparison> heap

private final int maxSize


#Methods
*Creates a new ComparisonHeap with the specified maximum size.
*@param maxSize Maximum number of comparisons to keep
public ComparisonHeap(int maxSize)

*Offers a comparison to the heap. The comparison will be added if:
*1. The heap is not yet full, or
*2. The comparison is better than the worst one in the heap
*@param comp The comparison to offer (will not be modified or stored directly)
*@return true if the comparison was added, false otherwise
public boolean offer(Comparison comp)

*Returns a sorted list of the top comparisons (best first)
public ArrayList<Comparison> toList()

*Returns the number of comparisons currently in the heap
public int size()

*Clears all comparisons from the heap
public void clear()

public Comparison worst()

</class ComparisonHeap>
<class ConservationModel>
*Models realistic genomic conservation variation using summed sine waves.
*Used to determine position-dependent mutation rates during sequence simulation.
*@author Brian Bushnell
*@contributor Isla Winglet
*@date May 30, 2025
public class ConservationModel

#Fields
private final float baseMutationRate

private final float[] amplitudes

private final float[] inversePeriods

private final float[] offsets

private static final float pi2=(float)(2 * Math.PI)


#Methods
public ConservationModel(Random randy)

public ConservationModel(float baseMutationRate, int numWaves, Random randy)

*Creates a conservation model with multiple sine waves for mutation rate variation.
*@param baseMutationRate Base mutation probability (0-1)
*@param numWaves Number of sine waves to combine
*@param maxAmplitude Maximum amplitude for sine wave oscillation
*@param randy Random number generator
*@param minPeriod Minimum period length in base pairs
*@param maxPeriod Maximum period length in base pairs
public ConservationModel(float baseMutationRate, int numWaves, float maxAmplitude, Random randy, int minPeriod, int maxPeriod)

*Calculates mutation probability at the given position.
*@param position Position in sequence (0-based)
*@return Mutation probability (0-1)
public float getMutationProbability(int position)

*Determines whether to mutate at the given position.
*@param position Position in sequence (0-based)
*@param randy Random number generator
*@return true if position should be mutated
public boolean shouldMutatePosition(int position, Random randy)

</class ConservationModel>
<class Contig>
public class Contig

#Fields
private int id=-1

public Cluster cluster=null

public final String name

public final String shortName

public final byte[] bases


#Methods
public Contig(String name_, byte[] bases_, int id_)

@Override public String name()

@Override public boolean isCluster()

@Override public Cluster toCluster()

boolean sameCluster(Bin b)

@Override public Cluster cluster()

@Override public void setID(int id_)

@Override public int id()

@Override public boolean isValid()

public void loadCounts()

*In fasta format
public void appendTo(ByteBuilder bb, int cluster)

@Override public long size()

@Override public Sketch toSketch(SketchMakerMini smm, Read r)

public final ByteBuilder toCov(ByteBuilder bb)

@Override public int numContigs()

@Override public Iterator<Contig> iterator()

</class Contig>
<class ContigRenamer>
*Renames contigs based on a sam file.
*Appends coverage (cov_#).
*Also appends taxid (taxid_#) if reads have taxids,
*and the contig does not already have a taxid.
*@author Brian Bushnell
*@date September 6, 2019
public class ContigRenamer

#Fields
*Sam input file path
private ArrayList<String> in=new ArrayList<String>()

*Contig input file path
private String ref=null

private String cami=null

*Primary output file path
private String out=null

*Override input file extension
private String extin=null

*Override output file extension
private String extout=null

*Number of reads processed
protected long readsProcessed=0

*Number of bases processed
protected long basesProcessed=0

*Number of reads retained
protected long readsOut=0

*Number of bases retained
protected long basesOut=0

*Quit after processing this many input reads; -1 means no limit
private long maxReads=-1

private String delimiter=" "

private boolean wipe=false

private boolean addDepth=true

private boolean addTid=true

*Threads dedicated to reading the sam file
private int streamerThreads=SamStreamer.DEFAULT_THREADS

public final SamFilter samFilter=new SamFilter()

public HashMap<String,Scaf> scafMap=new HashMap<String,Scaf>()

private final ReadWriteLock rwlock=new ReentrantReadWriteLock()

*Secondary input file
private final FileFormat ffref

*Primary output file
private final FileFormat ffout

*Print status messages to this output stream
private PrintStream outstream=System.err

*Print verbose messages
public static boolean verbose=false

*True if an error was encountered
public boolean errorState=false

*Overwrite existing output files
private boolean overwrite=true

*Append to existing output files
private boolean append=false

*Reads are output in input order
private boolean ordered=false


#Methods
*Code entrance from the command line.
*@param args Command line arguments
public static void main(String[] args)

*Constructor.
*@param args Command line arguments
public ContigRenamer(String[] args)

*Parse arguments from the command line
private Parser parse(String[] args)

*Add or remove .gz or .bz2 as needed
private void fixExtensions()

*Ensure files can be read and written
private void checkFileExistence()

*Adjust file-related static fields as needed for this program
private static void checkStatics()

*Ensure parameter ranges are within bounds and required parameters are set
private boolean validateParams()

*Create read streams and process all data
void process(Timer t)

private void processSam()

private void processCami()

private ConcurrentReadInputStream makeRefCris()

private SamStreamer makeStreamer(FileFormat ff)

private static HashMap<String,Scaf> makeScafMap()

private ConcurrentReadOutputStream makeCros()

*Spawn process threads
private void spawnThreads(SamStreamer ss)

@Override public final void accumulate(ProcessThread pt)

@Override public final boolean success()

private void processReference()

static String toShortName(String s)

static String toShortName(byte[] s)

private String rename(String old, int scaflen, long numericID)

@Override public final ReadWriteLock rwlock()

</class ContigRenamer>
<class ContigRenamer.ProcessThread>
*This class is static to prevent accidental writing to shared variables.
*It is safe to remove the static modifier.
class ContigRenamer.ProcessThread

#Fields
*Number of reads processed by this thread
protected long readsProcessedT=0

*Number of bases processed by this thread
protected long basesProcessedT=0

*True only if this thread has completed successfully
boolean success=false

*Shared input stream
private final SamStreamer ss

*Thread ID
final int tid


#Methods
ProcessThread(SamStreamer ss_, int tid_)

@Override public void run()

*Iterate through the reads
void processInner()

void processList(ListNum<SamLine> ln)

private void addLine(SamLine sl)

</class ContigRenamer.ProcessThread>
<class CrystalChamber>
*Recrystallization-based bin refinement using centroid clustering.
*Dissolves clusters and rebuilds them using iterative centroid assignment
*to find natural partitions that greedy algorithms might miss.
*"Like chemistry - sometimes you need to dissolve and re-precipitate
*to allow components to find their optimal, pure state." - Kairos
*@author UMP45
class CrystalChamber

#Fields
private final Oracle oracle

private final int maxIterations

private final float convergenceThreshold

private final float minSplitImprovement

private final Random random


#Methods
public CrystalChamber(Oracle oracle_)

*Creates a CrystalChamber refiner with specified Oracle and configurable random seed.
*@param oracle_ Oracle instance for contig similarity evaluation during clustering
*@param seed Random seed for deterministic centroid initialization and reproducible results
public CrystalChamber(Oracle oracle_, long seed)

@Override ArrayList<Bin> refine(Bin input)

*Performs iterative centroid-based clustering to separate contigs.
private ArrayList<Cluster> recrystallize(ArrayList<Contig> contigs, int k)

*Initialize centroids by finding most dissimilar contigs.
private ArrayList<Centroid> initializeCentroids(ArrayList<Contig> contigs, int k)

*Find the centroid nearest to given contig.
private int findNearestCentroid(Contig contig, ArrayList<Centroid> centroids)

*Calculate centroid of a group of contigs.
private Centroid calculateCentroid(ArrayList<Contig> contigs)

*Calculate distance between two centroids.
private float centroidDistance(Centroid a, Centroid b)

*Test whether Oracle would recommend merging the split clusters back together.
private boolean shouldMergeBack(Cluster a, Cluster b)

@Override ArrayList<structures.IntHashSet> refineToIntSets(Bin input)

</class CrystalChamber>
<class CrystalChamber2>
*Enhanced recrystallization-based bin refinement using iterative centroid clustering.
*Implements binary splitting (k=2) with farthest-first centroid initialization and assignment stability detection.
*The algorithm performs K-means-style clustering using Oracle similarity calculations instead of Euclidean distance,
*making it suitable for biological sequence similarity rather than numerical feature clustering.
*Uses fixed binary splits to avoid algorithmic complexity since recursive splitting can achieve multi-way partitions.
*The recrystallization process iteratively assigns contigs to the nearest centroid (highest similarity),
*then updates centroids to represent each cluster. Convergence is detected when assignments stabilize
*between iterations, providing faster termination than traditional centroid movement thresholds.
*"Perfect binary splits with better initialization - no need for complexity." - UMP45
*@author UMP45
*@author Brian Bushnell
class CrystalChamber2

#Fields
*Oracle instance for biological sequence similarity calculations during clustering and validation
private final Oracle oracle

*Maximum iterations allowed for centroid convergence before terminating clustering attempts
private final int maxIterations

*Traditional convergence threshold for centroid movement (unused in assignment stability detection)
private final float convergenceThreshold

*Minimum similarity difference required between clusters to justify split (prevents over-fragmentation)
private final float minSplitImprovement

*Random number generator with fixed seed for reproducible centroid initialization across runs
private final Random random

*Enable debugging output for split success rate analysis and similarity threshold monitoring
private final boolean debug

*Total number of split attempts across all inputs for debugging and performance statistics
private int splitAttempts

*Number of successful splits that passed quality thresholds for debugging and effectiveness analysis
private int successfulSplits


#Methods
*Creates enhanced CrystalChamber2 refiner with specified Oracle for similarity calculations.
*Initializes recrystallization parameters for iterative centroid-based clustering with binary splitting.
*Sets reasonable defaults for convergence (50 iterations max), split quality thresholds (0.1 minimum improvement),
*and reproducible random seed for consistent centroid initialization across runs.
*Enables debugging output to track split success rates and similarity thresholds.
*@param oracle_ Oracle instance for contig similarity evaluation during clustering
public CrystalChamber2(Oracle oracle_)

*Performs binary splitting refinement on input cluster using recrystallization algorithm.
*Attempts to separate the input cluster into two sub-clusters using iterative centroid-based clustering.
*First validates input size (minimum 4 contigs required for meaningful split), then performs recrystallization
*with k=2 binary splitting. Validates split quality using Oracle similarity calculations and split improvement thresholds.
*Returns null if input is too small, recrystallization fails, split quality is insufficient, or resulting clusters
*would be merged back together based on similarity scores.
*@param input Input cluster to refine through binary splitting
*@return ArrayList containing two refined clusters, or null if splitting failed or was not beneficial
@Override ArrayList<Bin> refine(Bin input)

*Determines optimal number of clusters based on cluster size heuristics.
*Uses simple rules to balance clustering effectiveness with computational complexity.
*Small clusters (size < 8) use binary splits to avoid over-fragmentation.
*Medium clusters (8-19) can support 3-way splits for better separation.
*Large clusters (20-49) use 4-way splits to handle complexity without excessive fragmentation.
*Very large clusters use up to 5-way splits or size/10, whichever is smaller.
*Note: Current implementation uses fixed k=2, making this method unused but preserved for future flexibility.
*@param contigs List of contigs to analyze for optimal k value determination
*@return Recommended number of clusters (2-5) based on input size heuristics
private int determineOptimalK(ArrayList<Contig> contigs)

*Performs iterative centroid-based clustering to separate contigs into k clusters using Oracle similarity.
*Implements K-means-style algorithm adapted for biological sequence similarity rather than Euclidean distance.
*Uses farthest-first centroid initialization to maximize initial separation, then iteratively assigns
*contigs to nearest centroids and updates centroid representatives. Enhanced convergence detection uses
*assignment stability (no contig reassignments) rather than traditional centroid movement thresholds.
*Fails if any cluster becomes empty during iterations or if initialization cannot find sufficient diversity.
*Creates proper Cluster objects with correct contig.cluster pointer updates to maintain data integrity.
*@param contigs List of contigs to partition into clusters using similarity-based assignments
*@param k Number of clusters to create (typically k=2 for binary splitting)
*@return ArrayList of k clusters with proper contig assignments, or null if clustering fails
private ArrayList<Cluster> recrystallize(ArrayList<Contig> contigs, int k)

*Initializes centroids using farthest-first strategy to maximize initial cluster separation.
*Selects first centroid randomly from available contigs, then iteratively chooses subsequent centroids
*to maximize the minimum distance (1 - similarity) to all previously selected centroids.
*This approach provides better initial separation than random selection and avoids the problematic
*K-means++ probabilistic weighting which doesn't work well with Oracle similarity functions.
*Each iteration examines all remaining contigs and selects the one most dissimilar to the closest
*existing centroid, creating well-separated initial cluster centers for effective convergence.
*@param contigs Available contigs for centroid selection and initialization
*@param k Number of centroids to initialize (must be <= contigs.size())
*@return ArrayList of k initialized centroids with maximum separation, or null if insufficient contigs
private ArrayList<Centroid> initializeCentroids(ArrayList<Contig> contigs, int k)

*Finds the centroid with highest similarity to the given contig for cluster assignment.
*Iterates through all available centroids and calculates Oracle similarity scores,
*returning the index of the centroid with maximum similarity. Uses "nearest" in the similarity sense
*(highest similarity score) rather than traditional distance minimization, since Oracle provides
*similarity rather than distance metrics. This assignment strategy ensures contigs are placed
*in clusters where they are most similar to the representative centroid.
*@param contig Contig to assign to the most similar centroid
*@param centroids Available centroids for similarity-based assignment
*@return Index of centroid with highest similarity to the input contig
private int findNearestCentroid(Contig contig, ArrayList<Centroid> centroids)

*Calculates centroid representative for a group of contigs using size-based selection.
*Uses the largest contig in the group as the centroid representative, under the assumption
*that larger contigs provide more stable similarity calculations and better represent
*the cluster's characteristics. This is a practical heuristic since true feature averaging
*is complex for biological sequences and may not improve clustering effectiveness.
*For single-contig groups, returns the contig itself as the centroid representative.
*The size-based selection provides deterministic centroid updates and avoids computational
*complexity of feature space averaging while maintaining reasonable clustering behavior.
*@param contigs Group of contigs requiring centroid representation for clustering
*@return Centroid object with largest contig as representative, or null if group is empty
private Centroid calculateCentroid(ArrayList<Contig> contigs)

*Checks if two assignment sets are identical for convergence detection in clustering iterations.
*Performs deep comparison of all contig assignments across all clusters to determine if
*assignments have stabilized between iterations. This provides more reliable convergence
*detection than traditional centroid movement thresholds, since it directly measures
*the clustering objective (stable assignments) rather than intermediate values.
*Uses element-wise comparison assuming assignment lists maintain consistent ordering.
*@param a First assignment set from current iteration
*@param b Second assignment set from previous iteration
*@return true if all assignments are identical (converged), false if any differences exist
private boolean assignmentsEqual(ArrayList<ArrayList<Contig>> a, ArrayList<ArrayList<Contig>> b)

*Creates deep copy of assignment sets for comparison in subsequent clustering iterations.
*Prevents accidental modification of stored assignments during convergence checking by creating
*independent copies of all cluster assignment lists. Essential for assignment stability detection
*since the original assignments will be cleared and rebuilt in each iteration.
*Only copies list structure and references, not the Contig objects themselves, which is sufficient
*for assignment comparison purposes and avoids unnecessary object duplication.
*@param original Original assignment set from current iteration to preserve
*@return Deep copy of assignment structure with independent ArrayList instances
private ArrayList<ArrayList<Contig>> deepCopyAssignments(ArrayList<ArrayList<Contig>> original)

@Override ArrayList<structures.IntHashSet> refineToIntSets(Bin input)

</class CrystalChamber2>
<class DataLoader>
public class DataLoader

#Fields
*Assembly path
String ref=null

String covIn=null

String netFileSmall="auto"

String netFileMid="auto"

String netFileLarge="auto"

private final ArrayList<String> covstats=new ArrayList<String>()

*Primary read input file path
private final ArrayList<String> readFiles=new ArrayList<String>()

boolean depthCalculated=false

boolean ignoreMissingContigs=false

long contigsLoaded=0

long basesLoaded=0

long contigsRetained=0

long basesRetained=0

private int bloomkbig=31

private int cbits=16

private int hashes=3

BinSketcher sketcher

int minContigToLoad=100

int minMapq=20

float minID=0.96f

float minMateID=0.97f

int maxSubs=999

float minEntropy=0

int tipLimit=100

int minAlignedBases=1

int gap=0

*Quit after processing this many input reads; -1 means no limit
private long maxReads=-1

IntLongHashMap sizeMap

IntHashMap[] graph

static int MAX_DEPTH_COUNT=999

static int MAX_EDGES_TO_PRINT=8

static boolean loadSamSerial=false

boolean makePairGraph=true

int numDepths=0

static boolean streamContigs=true

boolean errorState=false

static ArrayList<Contig> allContigs

final Timer phaseTimer=new Timer()

final PrintStream outstream


#Methods
DataLoader(PrintStream outstream_)

boolean parse(String arg, String a, String b)

static boolean looksLikeCovFile(String s)

*Adjust file-related static fields as needed for this program
private static void checkStatics()

void checkInput()

ArrayList<Contig> loadData()

int fixEdges(Contig c, int numContigs)

public ArrayList<Contig> loadContigs(String fname)

public ArrayList<Contig> loadAndProcessContigs(String fname, boolean parseCov)

public ArrayList<Contig> loadAndProcessContigsMT(String fname, boolean parseCov)

public ArrayList<Contig> loadAndProcessContigsST(String fname, boolean parseCov)

CellNet loadNetwork(String fname, String size, float cutoff, int samples)

int calculateDepth(ArrayList<Contig> contigs)

public static double calcDepthSum(ArrayList<Contig> contigs)

void validateEdges(ArrayList<Contig> contigs)

HashMap<String,Contig> toMap(Collection<Contig> list)

ArrayList<Contig> loadContigsST(String fname, int minlen)

Contig loadContig(Read r, int minlen)

BloomFilter makeBloomFilter(String in1, String in2)

void calcDepthFromCovStats(String fname, int sample, HashMap<String,Contig> contigMap, int minlen)

@Deprecated void calcDepthFromSam(FileFormat ff, int sample, HashMap<String,Contig> contigMap)

@Deprecated private void processSam(SamLineStreamer ss, int sample, HashMap<String,Contig> contigMap)

public void calcDepthFromHeader(Collection<Contig> list)

public static boolean parseAndSetDepth(Contig c, LineParserS1 lps, LineParserS4 lpt)

public void calcDepthFromBloomFilter(Collection<Contig> list, BloomFilter bf, int sample)

public static HashMap<String,FloatList> loadCovFile(String fname)

public void loadCovFile(String fname, ArrayList<Contig> contigs, int maxSamples)

public long trimEdges(ArrayList<Contig> contigs, int maxEdges, int minWeight, boolean reciprocal)

public long trimEdgesPass(ArrayList<Contig> contigs, int maxEdges, int minWeight, boolean reciprocal)

public static void writeCov(String fname, Collection<Contig> contigs, int numDepths, PrintStream outstream)

</class DataLoader>
<class EnsembleRefiner>
*Ensemble-based bin refinement combining multiple clustering strategies.
*Applies CrystalChamber, GraphRefiner, and EvidenceRefiner in parallel,
*then uses consensus voting to make robust refinement decisions.
*"Like a scientific peer review - get multiple expert opinions,
*then trust only the decisions supported by consensus." - EnsembleRefiner Philosophy
*@author UMP45
class EnsembleRefiner

#Fields
*Oracle instance for contig similarity calculations
private final Oracle oracle

*Individual refiner instances
private final CrystalChamber crystalRefiner

private final GraphRefiner graphRefiner

private final EvidenceRefiner evidenceRefiner

*Consensus threshold for co-occurrence decisions
private final float consensusThreshold

*Minimum number of methods that must agree
private final int minMethodsAgreeing

*Random number generator for deterministic consensus tie-breaking
private final Random random

*Enable debugging output
private final boolean debug

*Split attempt counter
private int splitAttempts

*Successful split counter
private int successfulSplits


#Methods
*Creates an EnsembleRefiner with specified Oracle for similarity calculations.
*@param oracle_ Oracle instance for contig similarity evaluation
public EnsembleRefiner(Oracle oracle_)

*Creates an EnsembleRefiner with specified Oracle and parameters.
*@param oracle_ Oracle instance for contig similarity evaluation
*@param params EnsembleRefiner-specific parameters
public EnsembleRefiner(Oracle oracle_, AbstractRefiner.EnsembleRefinerParams params)

@Override ArrayList<IntHashSet> refineToIntSets(Bin input)

@Override ArrayList<Bin> refine(Bin input)

*Applies all available refinement methods to the input cluster.
private ArrayList<RefinerResult> applyAllRefiners(Bin input, ArrayList<Contig> contigs)

*Converts Bin results to IntHashSet format for consensus building.
private ArrayList<IntHashSet> convertToIntHashSets(ArrayList<Bin> bins, ArrayList<Contig> contigs)

*Builds consensus clustering from multiple refiner results.
*Uses co-occurrence counting to identify robust cluster boundaries.
private ConsensusResult buildConsensus(ArrayList<RefinerResult> results, ArrayList<Contig> contigs)

*Grows a consensus cluster using connectivity threshold.
private void growConsensusCluster(int seed, IntHashSet cluster, boolean[] assigned, float[][] coOccurrence, float threshold)

*Converts IntHashSet consensus clusters to Cluster objects.
private ArrayList<Bin> convertToCluster(ArrayList<IntHashSet> clusterSets, ArrayList<Contig> contigs)

*Restores original cluster references when refinement fails or is rejected.
*Prevents cluster reference corruption similar to CrystalChamber fix.
private void restoreOriginalCluster(Cluster originalCluster)

</class EnsembleRefiner>
<class EvidenceRefiner>
*Evidence-based bin refinement using DBSCAN-style density clustering.
*Identifies dense regions of similar contigs separated by sparse boundaries,
*automatically determining cluster count and identifying outliers.
*"Like ecology - find dense populations separated by empty niches,
*automatically discovering natural community boundaries." - EvidenceRefiner Philosophy
*@author UMP45
class EvidenceRefiner

#Fields
*Oracle instance for contig similarity calculations
private final Oracle oracle

*DBSCAN epsilon parameter - similarity threshold for neighborhood
private final float epsilon

*DBSCAN minPoints parameter - minimum points for core region
private final int minPoints

*Minimum cluster size to be considered viable
private final int minClusterSize

*Random number generator for deterministic processing
private final Random random

*Enable debugging output
private final boolean debug

*Split attempt counter
private int splitAttempts

*Successful split counter
private int successfulSplits


#Methods
*Creates an EvidenceRefiner with specified Oracle for similarity calculations.
*@param oracle_ Oracle instance for contig similarity evaluation
public EvidenceRefiner(Oracle oracle_)

*Creates an EvidenceRefiner with specified Oracle and parameters.
*@param oracle_ Oracle instance for contig similarity evaluation
*@param params EvidenceRefiner-specific parameters
public EvidenceRefiner(Oracle oracle_, AbstractRefiner.EvidenceRefinerParams params)

@Override ArrayList<IntHashSet> refineToIntSets(Bin input)

@Override ArrayList<Bin> refine(Bin input)

*Performs DBSCAN clustering on contigs using Oracle similarity as distance metric.
private DBSCANResult performDBSCAN(ArrayList<Contig> contigs)

*Finds all contigs within epsilon similarity of the given contig using cached similarities.
private IntHashSet findNeighbors(int index, ArrayList<Contig> contigs, LongHashMap similarityCache)

*Expands cluster by recursively adding density-connected points.
private void expandCluster(int corePoint, IntHashSet neighbors, IntHashSet cluster, ContigStatus[] status, ArrayList<Contig> contigs, LongHashMap similarityCache)

*Validates that clusters have good internal cohesion and external separation.
private boolean hasGoodSeparation(ArrayList<IntHashSet> clusters, ArrayList<Contig> contigs)

*Calculates average internal similarity within a cluster.
private float calculateInternalSimilarity(IntHashSet cluster, ArrayList<Contig> contigs)

*Calculates average similarity between two different clusters.
private float calculateExternalSimilarity(IntHashSet cluster1, IntHashSet cluster2, ArrayList<Contig> contigs)

*Converts IntHashSet clusters to Cluster objects for compatibility.
private ArrayList<Bin> convertToCluster(ArrayList<IntHashSet> clusterSets, ArrayList<Contig> contigs)

*Restores original cluster references when refinement fails or is rejected.
*Prevents cluster reference corruption similar to CrystalChamber fix.
private void restoreOriginalCluster(Cluster originalCluster)

</class EvidenceRefiner>
<class FileRenamer>
*Renames files with top sketch hit taxid
public class FileRenamer

#Methods
public static void main(String[] args)

</class FileRenamer>
<class GeneTools>
public class GeneTools

#Fields
static String pgmFile=Data.findPath("?model.pgm")

static GeneModel pgm

static GeneCaller gCaller

static boolean quiet=false


#Methods
static GeneCaller makeGeneCaller()

static void loadPGM()

static void setMode(boolean r16, boolean r18, boolean r5, boolean r23, boolean trna, boolean cds)

</class GeneTools>
<class GradeBins>
*Grades bins.
*@author Brian Bushnell
*@date Feb 8, 2025
public class GradeBins

#Fields
private ArrayList<String> in=new ArrayList<String>()

private String taxIn=null

private String taxOut=null

private String tax=null

private String ref=null

private String hist=null

private String contamHist=null

private String ccplot=null

private String checkMFile=null

private String eukCCFile=null

private String camiFile=null

private String gtdbFile=null

private static String cov=null

private static String gffFile=null

private static String imgMapFile=null

private static String spectraFile=null

private static HashMap<String,ArrayList<GffLine>> gffMap

private static HashMap<String,FloatList> covMap

private String report=null

private LongList sizes=new LongList()

private ArrayList<BinStats> bins=new ArrayList<BinStats>()

private double contamScore=0

private double compltScore=0

private int minSize=1

private boolean loadMT=true

private static IntLongHashMap sizeMap

private static IntHashMap countMap

private static HashMap<String,CCLine> checkMMap

private static HashMap<String,CCLine> eukCCMap

private static HashMap<String,Integer> camiMap

private static HashMap<String,Lineage> gtdbMap

private static IntHashMap[] levelMaps

private static IntHashMap[] levelMapsHQ

private static IntHashMap[] levelMapsMQ

private static boolean runQuickClade=false

private static CladeIndex cladeIndex=null

private static boolean useTree=false

private static boolean callGenes=false

private static boolean useRNA=false

private static long maxReads=-1

private long readsProcessed=0

private long basesProcessed=0

private long totalSize=0

private long totalContigs=0

private long taxIDsIn=0

boolean overwrite=true

boolean success=true

private static java.io.PrintStream outstream=System.err

public static boolean verbose=false


#Methods
public GradeBins(String[] args)

static void makeLevelMaps()

static void loadCov()

static void loadGff()

static HashMap<String,String> loadImgMap(String fname)

static void loadSpectra()

void process(Timer t)

private void addTaxLevels(BinStats bin)

private void addTaxLevels(BinStats bin, Lineage lineage)

void printResults(ArrayList<BinStats> bins)

public static void printScore(ArrayList<BinStats> bins, long totalSize, long totalContigs, long taxIDsIn, boolean validation)

static String toScoreString(ArrayList<? extends Bin> bins, int minSize, IntLongHashMap sizeMap)

private static String toScoreString(ArrayList<BinStats> bins, long totalSize)

public static void printCleanDirty(ArrayList<BinStats> bins)

public static ArrayList<BinStats> loadST(ArrayList<String> in)

public ArrayList<BinStats> loadMT(ArrayList<String> in)

static Cluster loadCluster(String fname)

static void calcContam(String fname, Cluster c)

static ArrayList<BinStats> toStats(Collection<? extends Bin> bins, int minSize)

static void printClusterReport(Collection<? extends Bin> bins, int minSize, String fname)

static void printClusterReport(ArrayList<BinStats> bins, int minSize, String fname)

static void printTaxLevels(ArrayList<BinStats> bins, PrintStream outstream)

static void printBinQuality(Collection<? extends Bin> bins, int minSize, boolean useRNA, PrintStream outstream)

static void printBinQuality(ArrayList<BinStats> bins, int minSize, boolean useRNA, PrintStream outstream)

static void printL90FromBins(Collection<? extends Bin> bins, long basesLoaded)

static void printL90(Collection<BinStats> bins, long basesLoaded)

static void printL90(LongList list, long basesLoaded)

IntLongHashMap makeSizeMap(String fname)

private IntLongHashMap loadTaxIn(String fname)

private void writeTaxOut(String fname, IntLongHashMap sizeMap, IntHashMap countMap)

public static HashMap<String,CCLine> loadCheckM(String fname)

public static HashMap<String,CCLine> loadEukCC(String fname)

public static HashMap<String,Integer> loadCami(String fname)

public static HashMap<String,Lineage> loadGTDBDir(String fname)

public static boolean loadGTDBFile(String fname, HashMap<String,Lineage> map)

@Override public void accumulate(ProcessThread t)

@Override public ReadWriteLock rwlock()

@Override public boolean success()

static int callTax(Bin b)

static void callGenes(Bin b, GeneCaller gcall, BinStats bs)

static void annotate(Bin b, HashMap<String,ArrayList<GffLine>> map, BinStats bs)

</class GradeBins>
<class GradeBins.ProcessThread>
*This class is static to prevent accidental writing to shared variables.
*It is safe to remove the static modifier.
static class GradeBins.ProcessThread

#Fields
private final ArrayList<String> fnames

private final ArrayList<BinStats> bins

private final int tid

private final int threads

private final GeneCaller gCallerT

boolean success=false


#Methods
ProcessThread(ArrayList<String> fnames_, ArrayList<BinStats> bins_, int tid_, int threads_)

@Override public void run()

</class GradeBins.ProcessThread>
<class GraphRefiner>
*Graph-based bin refinement using modularity maximization.
*Constructs similarity graph and applies community detection algorithms
*to find natural cluster boundaries that may be missed by centroid methods.
*"Like sociology - find communities by analyzing the network of relationships,
*not just individual similarities." - GraphRefiner Philosophy
*@author UMP45
class GraphRefiner

#Fields
*Oracle instance for contig similarity calculations
private final Oracle oracle

*Minimum edge weight threshold for graph construction
private final float minEdgeWeight

*Maximum iterations for community detection
private final int maxIterations

*Random number generator for deterministic tie-breaking
private final Random random

*Enable debugging output
private final boolean debug

*Split attempt counter
private int splitAttempts

*Successful split counter
private int successfulSplits


#Methods
*Creates a GraphRefiner with specified Oracle for similarity calculations.
*@param oracle_ Oracle instance for contig similarity evaluation
public GraphRefiner(Oracle oracle_)

*Creates a GraphRefiner with specified Oracle and parameters.
*@param oracle_ Oracle instance for contig similarity evaluation
*@param params GraphRefiner-specific parameters
public GraphRefiner(Oracle oracle_, AbstractRefiner.GraphRefinerParams params)

@Override ArrayList<IntHashSet> refineToIntSets(Bin input)

@Override ArrayList<Bin> refine(Bin input)

*Builds weighted similarity graph from contigs.
*Only creates edges above minimum weight threshold.
private SimilarityGraph buildSimilarityGraph(ArrayList<Contig> contigs)

*Applies Louvain-style modularity maximization for community detection.
*Iteratively moves nodes to communities that maximize modularity gain.
private ArrayList<IntHashSet> detectCommunities(SimilarityGraph graph, ArrayList<Contig> contigs)

*Calculates modularity gain from moving a node to a different community.
private float calculateModularityGain(SimilarityGraph graph, int node, int newCommunity, int[] communities)

*Groups nodes by community ID into IntHashSet collections.
private ArrayList<IntHashSet> groupIntoCommunities(int[] communities)

*Calculates modularity score for given community structure.
private float calculateModularity(SimilarityGraph graph, ArrayList<IntHashSet> communities)

*Creates single community containing all nodes for baseline modularity calculation.
private ArrayList<IntHashSet> createSingleCommunity(ArrayList<Contig> contigs)

*Converts IntHashSet communities to Cluster objects for compatibility.
private ArrayList<Bin> convertToCluster(ArrayList<IntHashSet> communities, ArrayList<Contig> contigs)

*Restores original cluster references when refinement fails or is rejected.
*Prevents cluster reference corruption similar to CrystalChamber fix.
private void restoreOriginalCluster(Cluster originalCluster)

*Generates random permutation of indices 0..n-1 for unbiased processing.
private int[] generateRandomOrder(int n)

</class GraphRefiner>
<class GTDBLine>
public class GTDBLine

#Fields
String name

String domain

String phylum

String classname

String order

String family

String genus

String species

String classification


#Methods
public GTDBLine(byte[] line)

public GTDBLine(LineParser1 lptab, LineParser1 lpsemi)

public String getLevel(int level)

public String phylum()

public String classname()

public String order()

public String family()

public String genus()

public String species()

</class GTDBLine>
<class IDComparator>
public class IDComparator

#Fields
static final IDComparator comparator=new IDComparator()


#Methods
private IDComparator()

@Override public int compare(Bin a, Bin b)

</class IDComparator>
<class Key>
class Key

#Fields
int gcLevel

int covLevel

int covLevel2

private static final float maxDepth=1000000

private static float depthLevelMult=2f

private static float gcLevelWidth=0.02f

private static float gcLevelMult=1f / gcLevelWidth

private static int maxDepthLevel=quantizeDepth(maxDepth)


#Methods
static boolean parse(String arg, String a, String b)

public Key(float gc, float cov, float cov2)

public Key()

public Key set(Bin a)

public Key setLevel(int gcLevel_, int covLevel_, int covLevel2_)

public Key setValue(float gc, float cov, float cov2)

@Override public boolean equals(Object other)

public boolean equals(Key b)

@Override public int hashCode()

@Override public Key clone()

public static int quantizeDepth(float depth)

public static int quantizeGC(float gc)

public String toString()

static final void setGCMult(float f)

static final void setGCWidth(float f)

static final void setDepthWidth(float f)

static final void setDepthMult(float f)

</class Key>
<class KeyValue>
class KeyValue

#Fields
int key

int value


#Methods
KeyValue(int a_, int b_)

static ArrayList<KeyValue> toList(IntHashMap map)

@Override public int compareTo(KeyValue o)

</class KeyValue>
<class KmerProb>
public class KmerProb

#Fields
private static final int idxOffset=(int)Math.ceil(2 * Tools.log2(200))

public static float[][] matrix


#Methods
public static void load()

private static float[][] load(String fname)

*Probability that two sequences with this
*kmer frequency cosine difference come from the same genome.
*The length is the length of the shorter sequence.
*Genomes used for this were 5000 bacteria, <95% ANI,
*around 700m pairs.
*@param length
*@param dif
*@return
public static float prob(long length, float dif)

static int quantizeLength(long size)

static int dequantizeLength(int idx)

</class KmerProb>
<class Oracle>
public class Oracle

#Fields
Bin best=null

float score=-1

float topScore=-1

int bestIdx=-1

long fastComparisons=0

long trimerComparisons=0

long tetramerComparisons=0

long slowComparisons=0

long netComparisons=0

final float max3merDif0

final float max4merDif0

final float max5merDif0

final float maxDepthRatio0

final float maxGCDif0

final float maxProduct0

final float maxCovariance0

final float minKmerProb0

final int minEdgeWeight

final float stringency0

private FloatList vector

private CellNet networkSmall

private CellNet networkMid

private CellNet networkLarge

private IDAligner ssa=(SpectraCounter.call16S ? aligner.Factory.makeIDAligner() : null)

int taxlevel=TaxTree.SPECIES

boolean allowNoTaxID=true

boolean allowHalfTaxID=true

boolean useEdges=true

static ByteStreamWriter bsw

static boolean emitTP=true

static boolean emitFP=true

static boolean emitTN=true

static boolean emitFN=true

static int minEmitSize=0

static int maxEmitSize=2000000000

static double negativeEmitProb=1

static boolean printSizeInVector=false

static int printWeightInVector=1

static boolean printNetOutputInVector=false

static float minSSUID=0.98f

boolean verbose2=false


#Methods
public Oracle(float maxGCDif_, float maxDepthRatio_, float max3merDif_, float max4merDif_, float max5merDif_, float maxProduct_, float maxCovariance_, float minKmerProb_, int minEdgeWeight_)

public Oracle(float stringency, int minEdgeWeight_)

void clear()

*Higher is more similar
static final float similarity(float ratio_, float gcDif_, float simDif_, float covariance_, float kmerProb_, long edges_)

public final float similarity(Bin a, Bin b, float stringency0)

public static float edgeMult(long e1, long e2, long eT, float d1, float d2)

*Higher is more similar
private final float similarity(Bin a, Bin b, float maxGCDif, float maxDepthRatio, float max3merDif, float max4merDif, float max5merDif, float maxProduct, float maxCovariance, float minKmerProb)

final float ssuCompatibility(Bin a, Bin b)

final float runNetwork(Bin a, Bin b, long minEdges, long transEdges, float gcDif, float depthRatio, float covariance, float trimerDif, float tetramerDif, float pentamerDif, float kmerProb, float similarity, boolean includeAnswer)

static String header()

FloatList toVector(Bin a, Bin b, FloatList list, boolean includeAnswer)

FloatList toVector(Bin a, Bin b, long minEdges, long transEdges, float gcDif, float depthRatio, float covariance, float trimerDif, float tetramerDif, float pentamerDif, float kmerProb, float similarity, FloatList list, boolean includeAnswer)

private boolean taxaOK(int aTaxid, int bTaxid)

protected Oracle clone()

static boolean canEmitVector(Bin a, Bin b, float result)

void emitVector(Bin a, Bin b, ByteStreamWriter bsw)

static void emitVector(FloatList vector, ByteStreamWriter bsw)

private CellNet getNetwork(long size)

</class Oracle>
<class QuickBin>
*Prototype for metagenome contig binning.
*@author Brian Bushnell
*@date December 6, 2024
public class QuickBin

#Fields
*Primary output file path; should contain % symbol for bins
private String outPattern=null

private boolean writeChaff=false

*Override output file extension
private String extout=null

private String covOut=null

static String vectorOut=null

private String sizeHist=null

private String report=null

private ArrayList<Contig> contigList

boolean clusterByTaxid=false

boolean clusterByTetramer=true

boolean refineClusters=true

boolean processResidue=true

boolean reclusterClusters=false

boolean purifyClusters=true

boolean fuseClusters=true

int followEdge1Passes=0

int followEdge2Passes=5

float edgeStringency1=0.25f

float edgeStringency2=1.1f

float strictnessMult=1f

private long clustersWritten=0

private long contigsWritten=0

private long basesWritten=0

private DataLoader loader

private Binner binner

private long fastComparisonsCreate=0

private long slowComparisonsCreate=0

private long fastComparisonsRefine=0

private long slowComparisonsRefine=0

private long netComparisonsRefine=0

private long fastComparisonsEdge=0

private long midComparisonsEdge=0

private long slowComparisonsEdge=0

private long netComparisonsEdge=0

private long fastComparisonsResidue=0

private long slowComparisonsResidue=0

private long fastComparisonsPurify=0

private long slowComparisonsPurify=0

private final BinSketcher sketcher

private final ReadWriteLock rwlock=new ReentrantReadWriteLock()

private String[] originalArgs

*Print verbose messages
public static boolean verbose=false

*True if an error was encountered
public boolean errorState=false

*Overwrite existing output files
private boolean overwrite=true

*Append to existing output files
private boolean append=false


#Methods
*Code entrance from the command line.
*@param args Command line arguments
public static void main(String[] args)

*Constructor.
*@param args Command line arguments
public QuickBin(String[] args)

*Parse arguments from the command line
private Parser parse(String[] args)

private void reprocessArgs()

*Ensure parameter ranges are within bounds and required parameters are set
private boolean validateParams()

*Ensure files can be read and written
private void checkFileExistence()

*Create read streams and process all data
void process(Timer t)

static void printCC(ArrayList<Contig> contigs, int minSize, IntLongHashMap sizeMap)

static String formatString(String term, int len, long a, long b)

private void outputClusters(String pattern, ArrayList<? extends Bin> clusters, long minBases, int minContigs)

private void printBin(Bin a, ByteStreamWriter bsw, ByteBuilder bb, int id)

private void printCluster(Cluster a, ByteStreamWriter bsw, ByteBuilder bb, int id)

private void printContig(Contig c, ByteStreamWriter bsw, ByteBuilder bb, int id)

*Spawn process threads
private void spawnThreads(ConcurrentReadInputStream cris, ConcurrentReadOutputStream ros)

@Override public final void accumulate(ProcessThread pt)

@Override public final boolean success()

@Override public final ReadWriteLock rwlock()

</class QuickBin>
<class QuickBin.ProcessThread>
*This class is static to prevent accidental writing to shared variables.
*It is safe to remove the static modifier.
static class QuickBin.ProcessThread

#Fields
*Number of reads processed by this thread
protected long readsProcessedT=0

*Number of bases processed by this thread
protected long basesProcessedT=0

*Number of reads retained by this thread
protected long readsOutT=0

*Number of bases retained by this thread
protected long basesOutT=0

*True only if this thread has completed successfully
boolean success=false

*Shared input stream
private final ConcurrentReadInputStream cris

*Shared output stream
private final ConcurrentReadOutputStream ros

*Thread ID
final int tid


#Methods
ProcessThread(ConcurrentReadInputStream cris_, ConcurrentReadOutputStream ros_, int tid_)

@Override public void run()

*Iterate through the reads
void processInner()

void processList(ListNum<Read> ln)

*Process a read or a read pair.
*@param r1 Read 1
*@param r2 Read 2 (may be null)
*@return True if the reads should be kept, false if they should be discarded.
boolean processReadPair(Read r1, Read r2)

</class QuickBin.ProcessThread>
<class SamLoader>
public class SamLoader

#Fields
public PrintStream outstream=System.err

public long readsIn=0

public long readsUsed=0

public long basesIn=0

public long bytesIn=0

public int minMapq=4

public int minMateq=4

public float minID=0f

public float minMateID=0f

public int maxSubs=999

public int tipLimit=100

public float minEntropy=0

public int minAlignedBases=0

public boolean errorState=false


#Methods
public SamLoader(PrintStream outstream_)

@Deprecated public void load(ArrayList<String> fnames, HashMap<String,Contig> contigMap, IntHashMap[] graph)

*Spawn process threads
public void load(ArrayList<String> fnames, HashMap<String,Contig> contigMap, ArrayList<Contig> contigs, IntHashMap[] graph)

@Override public void accumulate(LoadThread t)

@Override public ReadWriteLock rwlock()

@Override public boolean success()

</class SamLoader>
<class SamLoader.LoadThread>
class SamLoader.LoadThread

#Fields
final String fname

final int sample

final HashMap<String,Contig> contigMap

final ArrayList<Contig> contigs

final IntHashMap[] graph

final EntropyTracker et=new EntropyTracker(5,80,false,minEntropy,true)

final int[] kmerCounts=new int[256]

long readsInT=0

long readsUsedT=0

long basesInT=0

long bytesInT=0

boolean success=false


#Methods
LoadThread(String fname_, int sample_, HashMap<String,Contig> contigMap_, ArrayList<Contig> contigs_, IntHashMap[] graph_)

@Override public void run()

private void runInner()

private void postprocess(long[] depthArray)

void processSam_Thread(SamLineStreamer ss, long[] depthArray)

private int calcAlignedBases(SamLine sl, int contigLen)

private boolean addSamLine(SamLine sl, long[] depthArray)

</class SamLoader.LoadThread>
<class ScoreComparator>
class ScoreComparator

#Fields
public static final ScoreComparator comparator=new ScoreComparator()


#Methods
private ScoreComparator()

</class ScoreComparator>
<class SimilarityMeasures>
*Mostly written by ChatGPT and modified by me
public class SimilarityMeasures

#Fields
private static final float root2=(float)Math.sqrt(2)

private static final float log2=(float)Math.log(2)

private static final float invRoot2=1 / root2

private static final float invLog2=1 / log2

public static boolean GC_COMPENSATED=false

public static boolean COSINE=true

public static boolean EUCLID=false

public static boolean ABSOLUTE=false

public static boolean JSD=false

public static boolean HELLINGER=false

public static boolean KST=false


#Methods
public static void main(String[] args)

public static boolean parse(String arg, String a, String b)

public static float[] calculateDifferenceVector(float[] a, float[] b)

public static float calculateDifferenceAverage(int[] a, int[] b)

public static float[] calculateDifferenceVector(int[] a, int[] b)

public static float cosineDifference(float[] a, float[] b)

public static float cosineSimilarity(float[] a, float[] b)

public static float cosineDifference(int[] a, int[] b)

public static float cosineDifference(int[] a, int[] b, float inva, float invb)

public static float cosineDifferenceCompensated(int[] a, int[] b, int k)

public static float cosineSimilarity(int[] a, int[] b, float inva, float invb)

public static float cosineDifference(long[] a, long[] b)

public static float cosineDifference(long[] a, long[] b, float inva, float invb)

public static float cosineSimilarity(long[] a, long[] b, float inva, float invb)

public static float cosineSimilarityCompensated(int[] a, int[] b, int k)

public static float[] compensate(int[] a, int k, int[] gcmap)

public static float[] compensate(long[] a, int k)

public static float cosineSimilarityCompensated(int[] a, int[] b, int k, int[] gcmap)

public static float euclideanDistance(float[] a, float[] b)

public static float euclideanDistance(int[] a, int[] b)

public static float euclideanDistance(int[] a, int[] b, float inva, float invb)

public static float euclideanDistance(long[] a, long[] b)

public static float euclideanDistance(long[] a, long[] b, float inva, float invb)

*@param a Contig kmer frequencies
*@param b Cluster kmer frequencies
*@return Score
static final float absDif(float[] a, float[] b)

*@param a Contig kmer frequencies
*@param b Cluster kmer frequencies
*@return Score
static final float absDifFloat(float[] a, float[] b)

public static float absDif(int[] a, int[] b)

static final float absDif(int[] a, int[] b, float inva, float invb)

public static float absDifComp(long[] a, long[] b, int k)

public static float absDif(long[] a, long[] b)

private static final float absDif(long[] a, long[] b, float inva, float invb)

public static float jensenShannonDivergence(float[] a, float[] b)

public static float jensenShannonDivergence(int[] a, int[] b)

public static float jensenShannonDivergence(int[] a, int[] b, float inva, float invb)

public static float hellingerDistance(float[] a, float[] b)

public static float hellingerDistance(int[] a, int[] b)

public static float hellingerDistance(int[] a, int[] b, float inva, float invb)

public static float hellingerDistance(long[] a, long[] b)

public static float hellingerDistance(long[] a, long[] b, float inva, float invb)

*This is a KS test for binned histograms, not raw values
public static float ksTest(float[] histogram1, float[] histogram2)

*This is a KS test for binned histograms, not raw values
public static float ksTest(int[] a, int[] b, float inva, float invb)

</class SimilarityMeasures>
<interface Sketchable>
public interface Sketchable

#Methods
public void setFrom(JsonObject jo)

public Sketch toSketch(SketchMakerMini smm, Read r)

public void setID(int id)

public int id()

public float gc()

public long size()

public int taxid()

public int numContigs()

public long sketchedSize()

public void clearTax()

</interface Sketchable>
<class SketchRecord>
public class SketchRecord

#Fields
int matches=-1

float completeness=-1

float contam=-1

float ani=-1

int taxid=-1

int genusTaxid=-1

String taxName=null


#Methods
public SketchRecord(JsonObject hit)

public void setFrom(JsonObject hit)

public ByteBuilder toBytes()

public ByteBuilder appendTo(ByteBuilder bb)

private static String shrink(String n)

</class SketchRecord>
<class SpectraCounter>
public class SpectraCounter

#Fields
public PrintStream outstream=System.err

public final boolean parseDepth

public final boolean parseTID

public final IntLongHashMap sizeMap

public int contigsLoaded=0

public long basesLoaded=0

public int contigsRetained=0

public long basesRetained=0

public boolean errorState=false

public static boolean calcEntropy=true

public static boolean calcEntropyFast=false

public static boolean calcStrandedness=true

public static boolean call16S=false

public static boolean call18S=false

public static int loadThreadsOverride=-1


#Methods
public SpectraCounter(PrintStream outstream_, boolean parseDepth_, boolean parseTID_, IntLongHashMap sizeMap_)

*Spawn process threads
public void makeSpectra(ArrayList<Contig> contigs, ConcurrentReadInputStream cris, int minlen)

@Override public void accumulate(LoadThread t)

@Override public ReadWriteLock rwlock()

@Override public boolean success()

</class SpectraCounter>
<class SpectraCounter.LoadThread>
class SpectraCounter.LoadThread

#Fields
final int tid

final int threads

final int minlen

final ArrayList<Contig> contigs

final ConcurrentReadInputStream cris

private EntropyTracker et

private GeneCaller caller

boolean success=false

int contigsProcessedT=0

long basesProcessedT=0

LineParserS1 lps=new LineParserS1('_')

LineParserS4 lpt=new LineParserS4(",,=,")

int contigsLoadedT=0

long basesLoadedT=0

int contigsRetainedT=0

long basesRetainedT=0


#Methods
LoadThread(ArrayList<Contig> contigs_, ConcurrentReadInputStream cris_, int minlen_, int tid_, int threads_)

@Override public void run()

private void runInner()

void runOnContigs()

void runOnCris()

Contig processRead(Read r)

Contig loadContig(Read r)

byte[][] callSSU(Read r)

void processContig(Contig c)

</class SpectraCounter.LoadThread>
